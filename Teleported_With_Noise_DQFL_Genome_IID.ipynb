{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/Quantum_Teleportation_With_Noise/blob/main/Teleported_With_Noise_DQFL_Genome_IID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdPhLlT9q3Np"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E_h9sD60D2Dh",
        "outputId": "97af7371-aa5d-4709-8396-83601fb97168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit-terra==0.24.1\n",
            "  Downloading qiskit_terra-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting qiskit-aer==0.12.0\n",
            "  Downloading qiskit_aer-0.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting qiskit-ibmq-provider==0.20.2\n",
            "  Downloading qiskit_ibmq_provider-0.20.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting qiskit==0.43.1\n",
            "  Downloading qiskit-0.43.1.tar.gz (9.6 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting qiskit-machine-learning==0.6.1\n",
            "  Downloading qiskit_machine_learning-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting rustworkx>=0.12.0 (from qiskit-terra==0.24.1)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from qiskit-terra==0.24.1) (2.0.2)\n",
            "Collecting ply>=3.10 (from qiskit-terra==0.24.1)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.11/dist-packages (from qiskit-terra==0.24.1) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.11/dist-packages (from qiskit-terra==0.24.1) (1.14.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.11/dist-packages (from qiskit-terra==0.24.1) (1.13.1)\n",
            "Collecting dill>=0.3 (from qiskit-terra==0.24.1)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-terra==0.24.1) (2.9.0.post0)\n",
            "Collecting stevedore>=3.0.0 (from qiskit-terra==0.24.1)\n",
            "  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting symengine<0.10,>=0.9 (from qiskit-terra==0.24.1)\n",
            "  Downloading symengine-0.9.2-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.11/dist-packages (from qiskit-ibmq-provider==0.20.2) (2.32.3)\n",
            "Collecting requests-ntlm<=1.1.0 (from qiskit-ibmq-provider==0.20.2)\n",
            "  Downloading requests_ntlm-1.1.0-py2.py3-none-any.whl.metadata (938 bytes)\n",
            "Collecting numpy>=1.17 (from qiskit-terra==0.24.1)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from qiskit-ibmq-provider==0.20.2) (2.3.0)\n",
            "Requirement already satisfied: websocket-client>=1.5.1 in /usr/local/lib/python3.11/dist-packages (from qiskit-ibmq-provider==0.20.2) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-ibmq-provider==0.20.2) (15.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-machine-learning==0.6.1) (1.6.1)\n",
            "Collecting fastdtw (from qiskit-machine-learning==0.6.1)\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-machine-learning==0.6.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->qiskit-terra==0.24.1) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.20.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.20.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.20.2) (2025.1.31)\n",
            "Collecting ntlm-auth>=1.0.2 (from requests-ntlm<=1.1.0->qiskit-ibmq-provider==0.20.2)\n",
            "  Downloading ntlm_auth-1.5.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=1.3 in /usr/lib/python3/dist-packages (from requests-ntlm<=1.1.0->qiskit-ibmq-provider==0.20.2) (3.4.8)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->qiskit-machine-learning==0.6.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->qiskit-machine-learning==0.6.1) (3.6.0)\n",
            "Collecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit-terra==0.24.1)\n",
            "  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.3->qiskit-terra==0.24.1) (1.3.0)\n",
            "Downloading qiskit_terra-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_aer-0.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_ibmq_provider-0.20.2-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_machine_learning-0.6.1-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.7/148.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m841.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_ntlm-1.1.0-py2.py3-none-any.whl (5.7 kB)\n",
            "Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.9.2-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ntlm_auth-1.5.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: qiskit, fastdtw\n",
            "  Building wheel for qiskit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qiskit: filename=qiskit-0.43.1-py3-none-any.whl size=8142 sha256=58bf847405a36ac3dbae07d5e9e0763a62c4656eb613ae6f30daec885a08833c\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/97/09/9b15af04f453e18fe251b496beca9c41f77dc53a8d0e978e57\n",
            "  Building wheel for fastdtw (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-py3-none-any.whl size=3610 sha256=d17bc375d1866d7fd631067784625365c0a6f068cf28011a89389b3aa6bb8315\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/8a/f6/fd3df9a9714677410a5ccbf3ca519e66db4a54a1c46ea95332\n",
            "Successfully built qiskit fastdtw\n",
            "Installing collected packages: ply, symengine, pbr, numpy, ntlm-auth, dill, stevedore, rustworkx, requests-ntlm, fastdtw, qiskit-terra, qiskit-machine-learning, qiskit-ibmq-provider, qiskit-aer, qiskit\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.4.0 fastdtw-0.3.4 ntlm-auth-1.5.0 numpy-1.23.5 pbr-6.1.1 ply-3.11 qiskit-0.43.1 qiskit-aer-0.12.0 qiskit-ibmq-provider-0.20.2 qiskit-machine-learning-0.6.1 qiskit-terra-0.24.1 requests-ntlm-1.1.0 rustworkx-0.16.0 stevedore-5.4.1 symengine-0.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "07339a607c7448cdb4e4ff7450841420"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install qiskit-terra==0.24.1 qiskit-aer==0.12.0 qiskit-ibmq-provider==0.20.2 qiskit==0.43.1 qiskit-machine-learning==0.6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whZ-t2scuRjm"
      },
      "source": [
        "We are going to implement telepotation to establish a communication between clients.\n",
        "\n",
        "Here are the integration steps:\n",
        "\n",
        "##Model Update Encoding:\n",
        "After a federated round, choose the best client’s model. Extract its parameter vector—representing the critical update—and encode each parameter as a quantum state. In our simplified version, we use an RX rotation on a qubit where the rotation angle is determined by the parameter value.\n",
        "\n",
        "##Quantum Teleportation:\n",
        "For each parameter (or for a group of parameters, if you build a multi‑qubit circuit), use a teleportation circuit. In our demonstration, we build a function that creates a teleportation circuit to “transfer” the encoded value. In practice, the protocol would need to generate an entangled pair between sender and receiver, perform a Bell measurement, and then use classical communication to apply the correction on the receiver’s side.\n",
        "\n",
        "##Update Transfer:\n",
        "Replace or update the global model’s parameters with the teleported values. Then distribute these updated parameters to all the participating clients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaeXy79ZCl0i",
        "outputId": "cb9ebc1a-cacb-4a99-e21e-739269e432d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Your runtime has 359.2 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Us3DV6Sfq6Mw"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "!pip install genomic-benchmarks\n",
        "!pip install qiskit qiskit_machine_learning qiskit_algorithms\n",
        "!pip install qiskit-aer\n",
        "!pip install qiskit\n",
        "!pip install qiskit_machine_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "m5M_eDObB6OI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# ─── 0. Capture the start timestamp ────────────────────────────────────────────\n",
        "start_ts   = datetime.now()\n",
        "# Format as “DD_MM_YYYY_HHMMSS” (no colons, so filesystem‑safe)\n",
        "start_str  = start_ts.strftime(\"%d_%m_%Y_%H%M%S\")\n",
        "\n",
        "# ─── 1. Experiment configuration ───────────────────────────────────────────────\n",
        "num_epochs = 10 #50\n",
        "max_train_iterations = 100\n",
        "samples_per_epoch=50 #10\n",
        "word_size = 40\n",
        "use_teleportation             = True   # or False/True\n",
        "use_noise =True\n",
        "'''\n",
        "num_epochs = 2 #50\n",
        "max_train_iterations = 100\n",
        "samples_per_epoch=10 #10\n",
        "word_size = 40\n",
        "use_teleportation             = False   # or False/True\n",
        "use_noise =True\n",
        "'''\n",
        "\n",
        "num_clients = 5 #8 test with teleport\n",
        "num_federated_layers = 10\n",
        "num_deep_unfolding_iterations = 10\n",
        "initial_learning_rate = 0.15\n",
        "meta_learning_rate=1e-4\n",
        "initial_perturbation = 0.15\n",
        "momentum = 0.95\n",
        "gradient_moving_avg = 0\n",
        "\n",
        "# Define federated learning with accuracy tracking\n",
        "num_features = 5\n",
        "global_model_weights, global_model_accuracy = {}, []\n",
        "clients_train_accuracies, clients_test_accuracies = [], []\n",
        "round_times = []\n",
        "overall_start = time.time()\n",
        "\n",
        "\n",
        "# ─── 2. Helper to make floats filesystem‑safe (“0.15” → “0p15”) ─────────────────\n",
        "def fmt(x: float) -> str:\n",
        "    return str(x).replace('.', 'p')\n",
        "\n",
        "# ─── 3. Build the hyperparameter string ───────────────────────────────────────\n",
        "param_str = (\n",
        "    f\"clients{num_clients}\"\n",
        "    f\"_layers{num_federated_layers}\"\n",
        "    f\"_du{num_deep_unfolding_iterations}\"\n",
        "    f\"_lr{fmt(initial_learning_rate)}\"\n",
        "    f\"_pert{fmt(initial_perturbation)}\"\n",
        ")\n",
        "\n",
        "# ─── 4. Date stamp and Teleport flag ───────────────────────────────────────────\n",
        "date_str    = datetime.now().strftime(\"%d_%m_%Y\")               # e.g. \"22_04_2025\"\n",
        "teleport_pl = \"Teleport\" if use_teleportation else \"NoTeleport\"\n",
        "noise = \"WithNoise\" if use_noise else \"NoNoise\"\n",
        "\n",
        "# ─── 5. Assemble filenames for each artifact ─────────────────────────────────\n",
        "drive_root = \"/content/drive/MyDrive\"\n",
        "\n",
        "\n",
        "#  a) Best‑client CSV\n",
        "best_client_csv_file = os.path.join(\n",
        "    drive_root,\n",
        "    f\"DQFL_Genome_IID_Best_Client_{date_str}_{teleport_pl}_{noise}_{param_str}.csv\"\n",
        ")\n",
        "\n",
        "#  b) Global‑accuracy CSV\n",
        "global_csv_file = os.path.join(\n",
        "    drive_root,\n",
        "    f\"DQFL_Genome_IID_Global_{date_str}_{teleport_pl}_{noise}_{param_str}.csv\"\n",
        ")\n",
        "\n",
        "#  c) Local CSV\n",
        "csv_file = os.path.join(\n",
        "    drive_root,\n",
        "    f\"DQFL_Genome_IID_Local_{date_str}_{teleport_pl}_{noise}_{param_str}.csv\"\n",
        ")\n",
        "\n",
        "validation_csv_file = os.path.join(\n",
        "    drive_root,\n",
        "    f\"DQFL_Genome_IID_validation_Loss_{date_str}_{teleport_pl}_{noise}_{param_str}.csv\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpAw5S3imZQW",
        "outputId": "7431b110-42e9-4935-b61a-71207f2873d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sample int the data_set variable: \n",
            "('ATAGTTGTTTACTAGATTTCGAAATTCATTAACGATTTGCAATGAAATAGTAAAAGTTTTGAAAAATGGAGCCAATTTTCAGTGAACCTGAATAGCAATGCACTCCGAAATATGCCGAATTTGCAAGTTTTGGACATTTCCAATAATGAAATTGTATTCCGACCTAGAGATGTCGATTTTCTCAAACATACTCCACATTT', 0)\n",
            "\n",
            "First 5 samples in the word_combinations dict.\n",
            "ATAGTTGTTTACTAGATTTCGAAATTCATTAACGATTTGC 1\n",
            "TAGTTGTTTACTAGATTTCGAAATTCATTAACGATTTGCA 2\n",
            "AGTTGTTTACTAGATTTCGAAATTCATTAACGATTTGCAA 3\n",
            "GTTGTTTACTAGATTTCGAAATTCATTAACGATTTGCAAT 4\n",
            "TTGTTTACTAGATTTCGAAATTCATTAACGATTTGCAATG 5\n",
            "First 5 samples of encoded data:\n",
            "First 5 samples of encoded shuffled data:\n",
            "First 5 samples of scaled encoded shuffled data:\n",
            "Length of np_train_data: 5000\n",
            "Length of np_test_data: 1000\n",
            "Client 0 Test Data Length: 200\n",
            "Client 1 Test Data Length: 200\n",
            "Client 2 Test Data Length: 200\n",
            "Client 3 Test Data Length: 200\n",
            "Client 4 Test Data Length: 200\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "#from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "from qiskit.primitives import BackendSampler\n",
        "from functools import partial\n",
        "from qiskit_aer import Aer\n",
        "\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "#from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from qiskit_algorithms.utils import algorithm_globals # Import algorithm_globals\n",
        "\n",
        "# Set random seed for reproducibility using algorithm_globals\n",
        "algorithm_globals.random_seed = 42  # Set seed globally\n",
        "\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word_combinations = defaultdict(int)\n",
        "iteration = 1\n",
        "for text, _ in data_set:\n",
        "    for i in range(len(text)):\n",
        "        word = text[i:i+word_size]\n",
        "        if word_combinations.get(word) is None:\n",
        "          word_combinations[word] = iteration\n",
        "          iteration += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"First sample int the data_set variable: \")\n",
        "print(data_set[0])\n",
        "\n",
        "print(\"\\nFirst 5 samples in the word_combinations dict.\")\n",
        "for key, value in list(word_combinations.items())[:5]:\n",
        "    print(key, value)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# Preprocess the training set\n",
        "np_data_set = []\n",
        "for i in range(len(data_set)):\n",
        "    sequence, label = data_set[i]\n",
        "    sequence = sequence.strip()  # Remove any leading/trailing whitespace\n",
        "    words = [sequence[i:i + word_size] for i in range(0, len(sequence), word_size)]  # Split the sequence into 4-letter words\n",
        "    int_sequence = np.array([word_combinations[word] for word in words])\n",
        "    data_point = {'sequence': int_sequence, 'label': label}\n",
        "    np_data_set.append(data_point)\n",
        "\n",
        "\n",
        "print(\"First 5 samples of encoded data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np.random.shuffle(np_data_set)\n",
        "print(\"First 5 samples of encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sequences = np.array([item['sequence'] for item in np_data_set])\n",
        "sequences = np.vstack(sequences)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "sequences_scaled = scaler.fit_transform(sequences)\n",
        "\n",
        "for i, item in enumerate(np_data_set):\n",
        "    item['sequence'] = sequences_scaled[i]\n",
        "\n",
        "print(\"First 5 samples of scaled encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np_train_data = np_data_set[:5000]\n",
        "np_test_data = np_data_set[-1000:]\n",
        "\n",
        "print(f\"Length of np_train_data: {len(np_train_data)}\")\n",
        "print(f\"Length of np_test_data: {len(np_test_data)}\")\n",
        "\n",
        "test_sequences = [data_point[\"sequence\"] for data_point in np_test_data]\n",
        "test_labels = [data_point[\"label\"] for data_point in np_test_data]\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "#---------------------------------------\n",
        "\n",
        "\n",
        "class Client:\n",
        "   def __init__(self, data, test_data):  # Add test_data to __init__\n",
        "        self.data = data\n",
        "        self.test_data = test_data  # Store test_data as an attribute\n",
        "        self.models = []\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "        self.primary_model = None\n",
        "\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "    clients = []\n",
        "    # Split test data across clients\n",
        "    test_samples_per_client = len(np_test_data) // num_clients\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_data = []\n",
        "        for j in range(num_epochs):\n",
        "            start_idx = (i * num_epochs * samples_per_epoch) + (j * samples_per_epoch)\n",
        "            end_idx = (i * num_epochs * samples_per_epoch) + ((j + 1) * samples_per_epoch)\n",
        "            client_data.append(np_train_data[start_idx:end_idx])\n",
        "\n",
        "        # Assign a subset of the test data to each client\n",
        "        test_start_idx = i * test_samples_per_client\n",
        "        test_end_idx = (i + 1) * test_samples_per_client\n",
        "        client_test_data = np_test_data[test_start_idx:test_end_idx]\n",
        "\n",
        "        # Create Client instance with both train and test data\n",
        "        clients.append(Client(client_data, client_test_data))\n",
        "\n",
        "    return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n",
        "\n",
        "# Verify test data distribution across clients\n",
        "for index, client in enumerate(clients):\n",
        "    print(f\"Client {index} Test Data Length: {len(client.test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "PTRjimz4C79j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f0ee0c-8494-4f61-91f9-fc4430d62c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BackendEstimator', 'BackendSampler', 'BaseEstimator', 'BaseSampler', 'Estimator', 'EstimatorResult', 'Sampler', 'SamplerResult', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'backend_estimator', 'backend_sampler', 'base', 'estimator', 'primitive_job', 'sampler', 'utils']\n"
          ]
        }
      ],
      "source": [
        "import qiskit.primitives\n",
        "print(dir(qiskit.primitives))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into validation and train set"
      ],
      "metadata": {
        "id": "zq-9f_pt_hd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split # Import the train_test_split function\n",
        "# Define global_seed before using it\n",
        "global_seed = 42  # You can choose any integer value for the seed\n",
        "\n",
        "# 5) Split into federated train vs central validation\n",
        "train_frac = 0.9\n",
        "train_list, central_val = train_test_split(\n",
        "    np_data_set, train_size=train_frac, random_state=global_seed,\n",
        "    stratify=[d['label'] for d in np_data_set]\n",
        ")\n",
        "X_val = np.stack([d['sequence'] for d in central_val])\n",
        "y_val = np.array([d['label'] for d in central_val])\n",
        "print(f\"Train for federated: {len(train_list)}, central val: {len(central_val)}\")\n",
        "\n",
        "\n",
        "np_train_data = train_list[:5000]\n",
        "np_test_data = np_test_data[-1000:]\n",
        "\n",
        "print(f\"Length of sample train data for FL: {len(np_train_data)}\")\n",
        "print(f\"Length of sample test data for FL: {len(np_test_data)}\")\n",
        "\n",
        "test_sequences = [data_point[\"sequence\"] for data_point in np_test_data]\n",
        "test_labels = [data_point[\"label\"] for data_point in np_test_data]\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------------\n",
        "# 6) Partition federated data among clients\n",
        "\n",
        "class Client:\n",
        "   def __init__(self, data, test_data):  # Add test_data to __init__\n",
        "        self.data = data\n",
        "        self.test_data = test_data  # Store test_data as an attribute\n",
        "        self.models = []\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "        self.primary_model = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_du6xqxq_g5S",
        "outputId": "24050251-aec7-4adb-b14a-643b7d58e78b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train for federated: 67500, central val: 7500\n",
            "Length of sample train data for FL: 5000\n",
            "Length of sample test data for FL: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build central validation arrays\n",
        "X_val = np.stack([d['sequence'] for d in central_val])\n",
        "y_val = np.array([d['label'] for d in central_val])"
      ],
      "metadata": {
        "id": "fKjg71O3A2xa"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIg_mUrp3_2J"
      },
      "source": [
        "Data Load and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "3OTrftC6uZd_"
      },
      "outputs": [],
      "source": [
        "def split_dataset_for_epochs(num_clients, num_epochs, train_data, test_data, samples_per_epoch):\n",
        "    \"\"\"\n",
        "    Split the dataset across multiple epochs and clients.\n",
        "\n",
        "    Args:\n",
        "        num_clients (int): Number of clients.\n",
        "        num_epochs (int): Number of epochs.\n",
        "        train_data (list): List of training data points.\n",
        "        test_data (list): List of test data points.\n",
        "        samples_per_epoch (int): Number of samples per epoch.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of Client objects with assigned data for each epoch.\n",
        "    \"\"\"\n",
        "    clients = []\n",
        "\n",
        "    # Split the training data across epochs and clients\n",
        "    train_samples_per_client = len(train_data) // num_clients\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_data_for_epochs = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            start_idx = (i * num_epochs * samples_per_epoch) + (epoch * samples_per_epoch)\n",
        "            end_idx = (i * num_epochs * samples_per_epoch) + ((epoch + 1) * samples_per_epoch)\n",
        "            client_data_for_epochs.append(train_data[start_idx:end_idx])\n",
        "\n",
        "        # Assign test data to each client\n",
        "        test_samples_per_client = len(test_data) // num_clients\n",
        "        test_start_idx = i * test_samples_per_client\n",
        "        test_end_idx = (i + 1) * test_samples_per_client\n",
        "        client_test_data = test_data[test_start_idx:test_end_idx]\n",
        "\n",
        "        # Create a Client instance with epoch-specific data\n",
        "        clients.append(Client(client_data_for_epochs, client_test_data))\n",
        "\n",
        "    return clients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "vHHwtTUF65Gp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "#from IPython.display import clear_output\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import log_loss\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Callback function to capture the loss values\n",
        "objective_func_vals = []  # Global list to store loss values\n",
        "learning_rates = []\n",
        "perturbations = []\n",
        "# Data structure for tracking per-client, per-layer objective function values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "o0CaYcz9FQw7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os  # For handling directories\n",
        "\n",
        "# Define the directory to save the plots\n",
        "output_dir = \"federated_round_plots\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "# Initialize a global variable to track the round number\n",
        "current_round = 1\n",
        "\n",
        "# Callback for visualization, gradient smoothing, and learning rate adjustment in deep unfolding\n",
        "def deep_unfolding_learning_rate_adjustment(parameters, obj_func_eval, gradients=None,round_number=0):\n",
        "    global gradient_moving_avg, learning_rates, perturbations,current_round\n",
        "\n",
        "    #clear_output(wait=True)\n",
        "\n",
        "    # Save the objective function value for visualization\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "\n",
        "    # If gradients are provided, smooth the gradient using momentum\n",
        "    if gradients is not None:\n",
        "        gradient_moving_avg = momentum * gradient_moving_avg + (1 - momentum) * gradients  # Apply moving average\n",
        "        delta_lr = 0.05 * gradient_moving_avg  # Adjust learning rate based on the smoothed gradient\n",
        "        delta_perturbation = 0.1 * gradient_moving_avg  # Adjust perturbation based on the same gradient\n",
        "    else:\n",
        "        delta_lr = 0  # No gradient info available in this iteration\n",
        "        delta_perturbation = 0\n",
        "\n",
        "    # Update learning rate and perturbation\n",
        "    if len(learning_rates) > 0:\n",
        "        new_lr = max(0.001, learning_rates[-1] + delta_lr)  # Ensure learning rate is positive and non-zero\n",
        "        new_perturbation = max(0.001, perturbations[-1] + delta_perturbation)  # Ensure perturbation is positive\n",
        "    else:\n",
        "        new_lr = initial_learning_rate\n",
        "        new_perturbation = initial_perturbation\n",
        "\n",
        "    learning_rates.append(new_lr)\n",
        "    perturbations.append(new_perturbation)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Visualization of learning rate and perturbation\n",
        "    plt.figure(figsize=(10, 12))  # Adjust figure size for better spacing\n",
        "\n",
        "    # Plot Objective Function Value\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals, label=\"Objective Function Value\", color='blue')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective Function Value\")\n",
        "    plt.title(\"Objective Function Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)  # Add grid for better readability\n",
        "\n",
        "    # Plot Learning Rate\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(range(len(learning_rates)), learning_rates, label=\"Learning Rate\", color='green')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Learning Rate\")\n",
        "    plt.title(\"Learning Rate Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Perturbation\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(range(len(perturbations)), perturbations, label=\"Perturbation\", color='red')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Perturbation\")\n",
        "    plt.title(\"Perturbation Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout(pad=3.0)  # Add padding between subplots\n",
        "    # Save the plot after each federated round\n",
        "    #plot_filename = os.path.join(output_dir, f\"federated_round_{current_round}.png\")\n",
        "    #plt.savefig(plot_filename)  # Save the figure\n",
        "    #plt.show()\n",
        "    plt.close()  # Close the plot to free memory\n",
        "\n",
        "    # Increment the round number for the next call\n",
        "    current_round += 1\n",
        "\n",
        "\n",
        "# Define the SPSA callback to capture gradients and update learning rate and perturbation dynamically\n",
        "def spsa_callback(nfev, parameters, obj_func_eval, stepsize, accept):\n",
        "    # Assuming `stepsize` contains gradient information or its approximation\n",
        "    gradients = stepsize\n",
        "    deep_unfolding_learning_rate_adjustment(parameters, obj_func_eval, gradients)\n",
        "\n",
        "# Custom SPSA optimizer with learnable learning rate and perturbation\n",
        "class LearnableLRPerturbationSPSA(SPSA):\n",
        "    def __init__(self, initial_lr=1e-4, initial_perturbation=0.01, lr_alpha=0.1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.lr = initial_lr  # Initial learning rate\n",
        "        self.perturbation = initial_perturbation  # Initial perturbation\n",
        "        self.lr_alpha = lr_alpha  # Learning rate and perturbation update speed\n",
        "\n",
        "    def _update_learning_rate_and_perturbation(self, gradient, obj_func_eval):\n",
        "        \"\"\"\n",
        "        Update both learning rate and perturbation based on gradient and objective function evaluation.\n",
        "        The learning rate increases if the objective function improves and decreases otherwise.\n",
        "        \"\"\"\n",
        "        # Use the gradient sign to determine if we should increase or decrease\n",
        "        grad_lr = np.sign(np.mean(gradient))  # Average gradient sign across parameters\n",
        "\n",
        "        if grad_lr > 0:  # Objective function is improving\n",
        "            self.lr += self.lr_alpha * abs(grad_lr)  # Increase learning rate\n",
        "            self.perturbation += self.lr_alpha * abs(grad_lr)  # Increase perturbation\n",
        "        else:  # Objective function is getting worse\n",
        "            self.lr -= self.lr_alpha * abs(grad_lr)  # Decrease learning rate\n",
        "            self.perturbation -= self.lr_alpha * abs(grad_lr)  # Decrease perturbation\n",
        "\n",
        "        # Ensure both learning rate and perturbation are positive\n",
        "        self.lr = max(0.001, self.lr)\n",
        "        self.perturbation = max(0.001, self.perturbation)\n",
        "\n",
        "    def step(self, gradient, obj_func_eval):\n",
        "        \"\"\"\n",
        "        Perform optimization step for both parameters, learning rate, and perturbation.\n",
        "        Use the objective function evaluation to dynamically adjust learning rate and perturbation.\n",
        "        \"\"\"\n",
        "        self._update_learning_rate_and_perturbation(gradient, obj_func_eval)\n",
        "        return super().step(gradient)  # Perform SPSA step for parameters\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the optimizer state (learning rates, perturbations, and gradient moving averages) for the next round.\n",
        "        \"\"\"\n",
        "        self.lr = initial_learning_rate\n",
        "        self.perturbation = initial_perturbation\n",
        "        self.gradient_moving_avg = 0  # Reset the moving average of the gradient\n",
        "        learning_rates.clear()  # Reset the learning rates history\n",
        "        perturbations.clear()  # Reset the perturbations history\n",
        "        objective_func_vals.clear()  # Clear the objective function history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "X46XXHW1s4tR"
      },
      "outputs": [],
      "source": [
        "# Create optimizer with learnable learning rate and perturbation\n",
        "spsa_optimizer = LearnableLRPerturbationSPSA(\n",
        "      maxiter=25, learning_rate=initial_learning_rate, perturbation=initial_perturbation, callback=spsa_callback, lr_alpha=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Rhf0Ft7CI2",
        "outputId": "74f2f471-2973-4fb0-a42c-7fcf276b144c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#======================================================\n",
        "# Initialize QNN model\n",
        "def initialize_model(num_features,initial_params):\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "    qc = feature_map.compose(ansatz)\n",
        "\n",
        "    # Create optimizer with learnable learning rate and perturbation\n",
        "    #spsa_optimizer = LearnableLRPerturbationSPSA(\n",
        "     #maxiter=25, learning_rate=initial_learning_rate, perturbation=initial_perturbation, callback=spsa_callback, lr_alpha=0.01\n",
        "#)\n",
        "    global spsa_optimizer\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2\n",
        "\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,\n",
        "        input_params=feature_map.parameters,\n",
        "        weight_params=ansatz.parameters\n",
        "    )\n",
        "\n",
        "\n",
        "    # Define the neural network classifier\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "      neural_network=sampler_qnn,\n",
        "      optimizer=spsa_optimizer,\n",
        "      loss='squared_error',\n",
        "      initial_point=initial_params,  # Initialize with the starting parameters\n",
        ")\n",
        "\n",
        "\n",
        "    return qnn_classifier\n",
        "\n",
        "#=====================================================\n",
        "from google.colab import drive\n",
        "import csv\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define the save path in Google Drive\n",
        "#csv_file = '/content/drive/My Drive/DQFL_Genome_IID_22_04_2025_Teleport.csv'\n",
        "\n",
        "# Step 3: Define headers for the CSV\n",
        "headers = [\"Federated Round\", \"Client Number\", \"Iteration\", \"Objective Function Value\",\n",
        "           \"Training Accuracy\", \"Test Accuracy\", \"Learning Rate\", \"Perturbation\"]\n",
        "\n",
        "# Open the CSV file and write headers if it's the first time writing to the file\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(headers)\n",
        "\n",
        "# Example of saving results for each federated round and client\n",
        "def save_results(federated_round, client_id, iteration, obj_func_val, train_acc, test_acc, learning_rate, perturbation):\n",
        "    with open(csv_file, mode='a', newline='') as file:  # Open file in append mode\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([federated_round, client_id, iteration, obj_func_val, train_acc, test_acc, learning_rate, perturbation])\n",
        "#=====================================================\n",
        "\n",
        "# 7) Validation loss helper\n",
        "def compute_validation_loss(model, X, y):\n",
        "    \"\"\"\n",
        "    Compute the average cross-entropy loss on (X,y) using the QNN's output probabilities.\n",
        "    Falls back to a direct forward call if predict_proba is unavailable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Preferred: scikit-like API if available\n",
        "        proba = model.predict_proba(X)\n",
        "    except AttributeError:\n",
        "        # Fallback: extract weights and call the QNN forward method\n",
        "        weights = model._fit_result.x\n",
        "        proba = model._neural_network.forward(X, weights)\n",
        "    return log_loss(y, proba)\n",
        "\n",
        "#====================================================\n",
        "# Federated learning loop per client\n",
        "def train_qnn_model(client_data, client_test_data, model=None, client_id=None, layer=None):\n",
        "\n",
        "    global learning_rates, perturbations, objective_func_vals\n",
        "    print(\"Client Data Structure:\")  # Add this line to print the structure\n",
        "    print(client_data)                # This line prints the actual data\n",
        "    print(type(client_data))           # This line prints the data type\n",
        "    #num_features = client_data[0][\"sequence\"].shape[0]\n",
        "\n",
        "    #initial_params = np.random.rand(RealAmplitudes(client_data.shape[1], reps=4).num_parameters)  # Initialize params\n",
        "    initial_params = np.random.rand(RealAmplitudes(len(client_data[0][\"sequence\"]), reps=3).num_parameters)\n",
        "\n",
        "    if model is None:\n",
        "        model = initialize_model(num_features, initial_params)\n",
        "\n",
        "    train_sequences = np.array([data_point[\"sequence\"] for data_point in client_data])\n",
        "    train_labels = np.array([data_point[\"label\"] for data_point in client_data])\n",
        "    test_sequences = np.array([data_point[\"sequence\"] for data_point in client_test_data])\n",
        "    test_labels = np.array([data_point[\"label\"] for data_point in client_test_data])\n",
        "\n",
        "    train_accuracies, test_accuracies, total_time = [], [], 0\n",
        "\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    # Deep Unfolding with multiple iterations\n",
        "    # Continue training with learned weights and adjust learning rate based on performance and gradients.\n",
        "    total_time = 0\n",
        "    current_params = initial_params  # Start with the initial parameters\n",
        "\n",
        "    for i in range(num_deep_unfolding_iterations):\n",
        "        print(\"\\n\")\n",
        "        print(f\"Deep Unfolding Iteration {i+1}/{num_deep_unfolding_iterations}\")\n",
        "        start_time = time.time()\n",
        "        model.fit(train_sequences, train_labels)\n",
        "        end_time = time.time()\n",
        "        total_time += end_time - start_time\n",
        "\n",
        "        # After training, retrieve the updated parameters from the optimizer\n",
        "        current_params = model.weights\n",
        "        print(f\"Trained parameters after iteration {i+1}: {current_params}\")\n",
        "\n",
        "        # Store final weights and learning rate for next round\n",
        "        final_learning_rate = learning_rates[-1]\n",
        "        final_perturbation = perturbations[-1]\n",
        "\n",
        "        # Evaluate the model performance\n",
        "        train_accuracy = model.score(train_sequences, train_labels)\n",
        "        test_accuracy = model.score(test_sequences, test_labels)\n",
        "\n",
        "        # Store accuracies for future reference\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "\n",
        "        # Write the results to the CSV file\n",
        "        save_results(layer, client_id, i+1, objective_func_vals[-1], train_accuracy , test_accuracy , final_learning_rate, final_perturbation)\n",
        "\n",
        "        #with open(csv_file, mode='a', newline='') as file:\n",
        "          #writer = csv.writer(file)\n",
        "         #writer.writerow([i+1, objective_func_vals[-1], train_accuracy * 100, test_accuracy * 100, final_learning_rate, final_perturbation])\n",
        "\n",
        "        # Update the learning rate for the next iteration based on gradients from SPSA\n",
        "        spsa_optimizer.learning_rate = learning_rates[-1]\n",
        "        model.initial_point = current_params\n",
        "\n",
        "        # Log performance\n",
        "        print(f\"Iteration {i+1} - Learning Rate: {final_learning_rate:.6f}\")\n",
        "        print(f\"Iteration {i+1} - Training Accuracy: {train_accuracy:.2f}\")\n",
        "        print(f\"Iteration {i+1} - Test Accuracy: {test_accuracy :.2f}\")\n",
        "\n",
        "    return model, train_accuracy, test_accuracy, total_time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGhtrwV-DDHP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "AkLnHZoPTwB6"
      },
      "outputs": [],
      "source": [
        "# Step to empty the CSV file before starting a new run\n",
        "def clear_csv_file():\n",
        "    \"\"\"\n",
        "    Clears the CSV file by overwriting it with headers or leaving it blank.\n",
        "    \"\"\"\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Uncomment the next line to write headers for the new run\n",
        "        writer.writerow(headers)\n",
        "        # Leave it blank if you prefer not to include headers\n",
        "        # pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_global_accuracy(model, val_sequences, val_labels):\n",
        "    \"\"\"\n",
        "    Evaluate the accuracy of the given model on the test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model: The trained model to evaluate.\n",
        "        num_features: The number of features in each data sample.\n",
        "        test_sequences: A list or array of test input data (features).\n",
        "        test_labels: A list or array of true labels corresponding to the test data.\n",
        "\n",
        "    Returns:\n",
        "        float: The accuracy of the model as a percentage.\n",
        "    \"\"\"\n",
        "    global_accuracy = model.score(val_sequences, val_labels)\n",
        "    return global_accuracy\n"
      ],
      "metadata": {
        "id": "W-OXFZ_7BYPJ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "zpK0oXUHzPtm"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, test_sequences, test_labels):\n",
        "    \"\"\"\n",
        "    Evaluate the accuracy of the given model on the test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model: The trained model to evaluate.\n",
        "        num_features: The number of features in each data sample.\n",
        "        test_sequences: A list or array of test input data (features).\n",
        "        test_labels: A list or array of true labels corresponding to the test data.\n",
        "\n",
        "    Returns:\n",
        "        float: The accuracy of the model as a percentage.\n",
        "    \"\"\"\n",
        "    test_accuracy = model.score(test_sequences, test_labels)\n",
        "    return test_accuracy\n",
        "\n",
        "# Function to extract numerical values of parameters\n",
        "def extract_param_values(model):\n",
        "    #param_values = []\n",
        "    # Loop through each parameter in the circuit and get its bound value\n",
        "    # Retrieve the circuit from the neural network\n",
        "    circuit = model.neural_network.circuit\n",
        "\n",
        "    # Extract the parameter values bound to the circuit\n",
        "    # Use enumerate to get both index and parameter\n",
        "    param_values = {param: circuit.parameters[i] for i, param in enumerate(circuit.parameters)}\n",
        "    return param_values\n",
        "#def set_param_values(model, param_values):\n",
        "    # Retrieve the circuit from the neural network\n",
        "    #circuit = model.neural_network.circuit\n",
        "\n",
        "    # Use assign_parameters to update the parameter values\n",
        "    #circuit.assign_parameters(param_values, inplace=True)\n",
        "# Function to set numerical values of parameters back into the circuit\n",
        "def set_param_values(model, param_values):\n",
        "    # Assign the averaged parameter values back to the circuit\n",
        "    parameter_dict = {param: value for param, value in zip(model.neural_network.circuit.parameters, param_values)}\n",
        "    model.neural_network.circuit.assign_parameters(parameter_dict)\n",
        "\n",
        "\n",
        "# Manually average the numerical values of the parameters across clients\n",
        "def manual_average_weights(epoch_weights):\n",
        "    # Initialize a list to store the summed weights (initialize with zeros)\n",
        "    num_weights = len(epoch_weights[0])  # Number of weights in the model\n",
        "    num_clients = len(epoch_weights)  # Number of clients\n",
        "\n",
        "    # Initialize sum of weights to zero (assuming NumPy array or list of weights)\n",
        "    summed_weights = np.zeros(num_weights)\n",
        "\n",
        "    # Sum the weights from all clients\n",
        "    for client_weights in epoch_weights:\n",
        "        summed_weights += np.array(client_weights)\n",
        "\n",
        "    # Compute the average by dividing the summed weights by the number of clients\n",
        "    average_weights = summed_weights / num_clients\n",
        "\n",
        "    return average_weights\n",
        "\n",
        "def create_model_with_weights(weights):\n",
        "    initial_params = np.random.rand(RealAmplitudes(num_features, reps=3).num_parameters)\n",
        "    model = initialize_model(num_features,weights)\n",
        "    #set_param_values(model, weights)  # Assign global weights to the model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "MuhZZtnnzmV5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to save accuracies to CSV\n",
        "def save_accuracies_to_csv(global_accuracies, clients_train_accuracies, clients_test_accuracies, filename='accuracies.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the header row\n",
        "        header = ['Epoch', 'Global Accuracy']\n",
        "        for i in range(len(clients_train_accuracies[0])):  # Assuming all clients have the same number of records\n",
        "            header.append(f'Client {i} Train Accuracy')\n",
        "            header.append(f'Client {i} Test Accuracy')\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # Write the accuracy data for each epoch\n",
        "        for epoch in range(len(global_accuracies)):\n",
        "            row = [epoch, global_accuracies[epoch]]  # Start with epoch and global accuracy\n",
        "            for client_index in range(len(clients_train_accuracies[epoch])):\n",
        "                row.append(clients_train_accuracies[epoch][client_index])  # Add train accuracy for client\n",
        "                row.append(clients_test_accuracies[epoch][client_index])   # Add test accuracy for client\n",
        "            writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ez5A1HJ_DS5y"
      },
      "outputs": [],
      "source": [
        "clients = split_dataset_for_epochs(num_clients, num_epochs, np_train_data, np_test_data, samples_per_epoch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "3SPoEA6uDQmh",
        "outputId": "e9cf34da-b7de-4665-d5c7-cb750bf47dea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def plot_label_distribution(clients):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for idx, client in enumerate(clients):\n",
        "        labels = [sample['label'] for epoch in client.data for sample in epoch]\n",
        "        label_counts = Counter(labels)\n",
        "        labels_sorted = sorted(label_counts.keys())\n",
        "        counts = [label_counts[label] for label in labels_sorted]\n",
        "\n",
        "        plt.bar(\n",
        "            [str(label) + f\"_C{idx}\" for label in labels_sorted],\n",
        "            counts,\n",
        "            label=f'Client {idx}'\n",
        "        )\n",
        "\n",
        "    plt.xlabel(\"Label per Client\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Label Distribution per Client (Training Data)\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_label_distribution(clients)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5SZqeD8-eNP"
      },
      "source": [
        "#Start Implementing Telepotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBJUfKSh-Qk7"
      },
      "source": [
        "# A. Implementing a Simplified Teleportation for a Single Parameter\n",
        "\n",
        "Add a helper function (or functions) to encode and “teleport” one parameter. In a real implementation we would need to work with multi‑qubit circuits and extract continuous (analog) variables via state tomography. Here we simulate with a single‑qubit circuit where we assume that the parameter is recovered perfectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Wym4G3FfAX"
      },
      "source": [
        "Without adding noise model run as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZAMXy4GFkQ1"
      },
      "source": [
        "Add noise model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit import QuantumCircuit, Aer, execute\n",
        "from qiskit.visualization import plot_histogram\n",
        "from qiskit.providers.aer.noise import NoiseModel, errors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def teleport_parameter(val, show_histogram=False, use_noise=False):\n",
        "    \"\"\"\n",
        "    Simulated teleportation of a single parameter with an option to add noise.\n",
        "\n",
        "    Args:\n",
        "        val (float): The parameter value to teleport.\n",
        "        show_histogram (bool): If True, display the histogram of measurement outcomes.\n",
        "        use_noise (bool): If True, execute the circuit with an added noise model.\n",
        "\n",
        "    Returns:\n",
        "        float: The parameter value (assumed to be perfectly transmitted ideally).\n",
        "    \"\"\"\n",
        "    qc = QuantumCircuit(3, 2)\n",
        "\n",
        "    # --- Encode the parameter on qubit 0 ---\n",
        "    qc.rx(val, 0)\n",
        "\n",
        "    # --- Prepare an entangled pair ---\n",
        "    qc.h(1)\n",
        "    qc.cx(1, 2)\n",
        "\n",
        "    # --- Bell-State Measurement ---\n",
        "    qc.cx(0, 1)\n",
        "    qc.h(0)\n",
        "    qc.measure([0, 1], [0, 1])\n",
        "\n",
        "    # --- Conditional Corrections on qubit 2 ---\n",
        "    qc.cx(1, 2).c_if(qc.cregs[0], 1)\n",
        "    qc.cz(0, 2).c_if(qc.cregs[0], 2)\n",
        "\n",
        "    backend = Aer.get_backend('qasm_simulator')\n",
        "\n",
        "    # Define a noise model\n",
        "    noise_model = None\n",
        "    if use_noise:\n",
        "      # define matching errors ,Define separate error objects for your single‑ and two‑qubit gates.\n",
        "      #Attach each error only to gates of the matching size.\n",
        "       #1% depolarizing on single‑qubit gates\n",
        "        one_q_error = errors.depolarizing_error(0.01, 1)\n",
        "        # e.g. 2% depolarizing on two‑qubit gates ( can choose a different rate)\n",
        "        two_q_error = errors.depolarizing_error(0.02, 2)\n",
        "\n",
        "        noise_model = NoiseModel()\n",
        "        noise_model.add_all_qubit_quantum_error(one_q_error, ['rx', 'h'])\n",
        "        noise_model.add_all_qubit_quantum_error(two_q_error, ['cx'])\n",
        "\n",
        "        # Add a depolarizing error to single-qubit gates with probability 1%\n",
        "        #error = errors.depolarizing_error(0.01, 1)\n",
        "        #oise_model.add_all_qubit_quantum_error(error, ['rx', 'h', 'cx'])\n",
        "\n",
        "    result = execute(qc, backend, shots=1024, noise_model=noise_model).result()\n",
        "    counts = result.get_counts(qc)\n",
        "\n",
        "    if show_histogram:\n",
        "        plot_histogram(counts)\n",
        "        plt.show()\n",
        "\n",
        "    # In an ideal simulation, we assume perfect transmission\n",
        "    # If noise is applied,  can compare the output statistics.\n",
        "    return val\n",
        "\n",
        "def teleport_parameters(params, show_histogram=False, use_noise=False):\n",
        "    \"\"\"\n",
        "    Applies the teleportation process to a vector of parameters.\n",
        "\n",
        "    Each parameter is processed individually. The optional noise parameter\n",
        "    allows you to simulate realistic transmission errors.\n",
        "\n",
        "    Args:\n",
        "        params (array-like): A vector of parameters.\n",
        "        show_histogram (bool): If True, display the histogram for each parameter.\n",
        "        use_noise (bool): If True, simulate noise in the circuit execution.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The array of transmitted parameter values.\n",
        "    \"\"\"\n",
        "    teleported = [teleport_parameter(p, show_histogram, use_noise) for p in params]\n",
        "    return np.array(teleported)\n"
      ],
      "metadata": {
        "id": "l6h2eGcIUhlE"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDXyHXiWFpE-"
      },
      "source": [
        "Implement entanglement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "QiEFmACVFsAz"
      },
      "outputs": [],
      "source": [
        "def entanglement_parameter(val, show_histogram=False, use_noise=False):\n",
        "    \"\"\"\n",
        "    Simulates parameter transfer using entanglement only (without full teleportation).\n",
        "\n",
        "    Args:\n",
        "        val (float): The parameter value to encode.\n",
        "        show_histogram (bool): If True, display measurement histogram.\n",
        "        use_noise (bool): If True, simulate noise on the circuit.\n",
        "\n",
        "    Returns:\n",
        "        float: A degraded version of the parameter, simulating fidelity loss.\n",
        "    \"\"\"\n",
        "    qc = QuantumCircuit(3, 1)\n",
        "\n",
        "    # Encode the parameter on qubit 0.\n",
        "    qc.rx(val, 0)\n",
        "\n",
        "    # Create an entangled pair between qubits 1 and 2.\n",
        "    qc.h(1)\n",
        "    qc.cx(1, 2)\n",
        "\n",
        "    # Instead of performing a full teleportation, measure qubit 2 directly.\n",
        "    qc.measure(2, 0)\n",
        "\n",
        "    backend = Aer.get_backend('qasm_simulator')\n",
        "    noise_model = None\n",
        "    if use_noise:\n",
        "        noise_model = NoiseModel()\n",
        "        error1 = errors.depolarizing_error(0.01, 1) # Depolarizing error for 1-qubit gates\n",
        "        error2 = errors.depolarizing_error(0.02, 2) # Depolarizing error for 2-qubit gates\n",
        "\n",
        "        # Add errors specifically to their respective gate types\n",
        "        noise_model.add_all_qubit_quantum_error(error1, ['rx', 'h'])\n",
        "        noise_model.add_all_qubit_quantum_error(error2, ['cx'])\n",
        "\n",
        "    result = execute(qc, backend, shots=1024, noise_model=noise_model).result()\n",
        "    counts = result.get_counts(qc)\n",
        "\n",
        "    if show_histogram:\n",
        "        plot_histogram(counts)\n",
        "        plt.show()\n",
        "\n",
        "    # Simulate degradation—for instance, output 90% of the original parameter.\n",
        "    degraded_val = 0.9 * val\n",
        "    return degraded_val\n",
        "\n",
        "def entanglement_parameters(params, show_histogram=False, use_noise=False):\n",
        "    entangled = [entanglement_parameter(p, show_histogram, use_noise) for p in params]\n",
        "    return np.array(entangled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5rdA5ac-mjc"
      },
      "source": [
        "#B. Integrating Teleportation into the Federated Learning Loop\n",
        "Within our federated learning loop, once we have determined the best client's model and before updating all clients, implement to “teleport” the model’s parameter update.\n",
        "\n",
        "#More  implement ->error correction, state tomography, and the complications of multi‑qubit encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTt4DwZj7Is9"
      },
      "source": [
        "Federated Learning Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmYmJR4_7Hux",
        "outputId": "eb73487e-2a44-4397-f35e-50698ff8d3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 200\n",
            "Client 2:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 200\n",
            "Client 3:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 200\n",
            "Client 4:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 200\n",
            "Client 5:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 200\n",
            "Client 1:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 200\n",
            "  Number of features in a sequence: 5\n",
            "Client 2:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 200\n",
            "  Number of features in a sequence: 5\n",
            "Client 3:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 200\n",
            "  Number of features in a sequence: 5\n",
            "Client 4:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 200\n",
            "  Number of features in a sequence: 5\n",
            "Client 5:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 200\n",
            "  Number of features in a sequence: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Fed_Epoch: 0\n",
            "\n",
            "\n",
            "Fed_Epoch 0, Client 1:\n",
            "Training data for epoch 0: 50\n",
            "Client Data Structure:\n",
            "[{'sequence': array([0.88804694, 0.88804694, 0.88804694, 0.88804694, 0.88804694]), 'label': 1}, {'sequence': array([0.0412278, 0.0412278, 0.0412278, 0.0412278, 0.0412278]), 'label': 0}, {'sequence': array([0.91013273, 0.91013273, 0.91013273, 0.91013273, 0.91013273]), 'label': 1}, {'sequence': array([0.34559471, 0.34559471, 0.34559471, 0.34559471, 0.34559471]), 'label': 0}, {'sequence': array([0.25064396, 0.25064396, 0.25064396, 0.25064396, 0.25064396]), 'label': 0}, {'sequence': array([0.88504344, 0.88504344, 0.88504344, 0.88504344, 0.88504344]), 'label': 1}, {'sequence': array([0.15576445, 0.15576445, 0.15576445, 0.15576445, 0.15576445]), 'label': 0}, {'sequence': array([0.837541, 0.837541, 0.837541, 0.837541, 0.837541]), 'label': 1}, {'sequence': array([0.61626013, 0.61626013, 0.61626013, 0.61626013, 0.61626013]), 'label': 1}, {'sequence': array([0.67925209, 0.67925209, 0.67925209, 0.67925209, 0.67925209]), 'label': 1}, {'sequence': array([0.33779045, 0.33779045, 0.33779045, 0.33779045, 0.33779045]), 'label': 0}, {'sequence': array([0.32852296, 0.32852296, 0.32852296, 0.32852296, 0.32852296]), 'label': 0}, {'sequence': array([0.52399814, 0.52399814, 0.52399814, 0.52399814, 0.52399814]), 'label': 1}, {'sequence': array([0.23087267, 0.23087267, 0.23087267, 0.23087267, 0.23087267]), 'label': 0}, {'sequence': array([0.35634847, 0.35634847, 0.35634847, 0.35634847, 0.35634847]), 'label': 0}, {'sequence': array([0.56265612, 0.56265612, 0.56265612, 0.56265612, 0.56265612]), 'label': 1}, {'sequence': array([0.6280228 , 0.6280228 , 0.62802193, 0.62802193, 0.62802193]), 'label': 1}, {'sequence': array([0.99682735, 0.99682735, 0.99682735, 0.99682735, 0.99682735]), 'label': 1}, {'sequence': array([0.31984475, 0.31984475, 0.31984475, 0.31984475, 0.31984475]), 'label': 0}, {'sequence': array([0.69604518, 0.69604518, 0.69604518, 0.69604518, 0.69604518]), 'label': 1}, {'sequence': array([0.32288086, 0.32288086, 0.32288086, 0.32288086, 0.32288086]), 'label': 0}, {'sequence': array([0.63142649, 0.63142649, 0.63142649, 0.63142649, 0.63142649]), 'label': 1}, {'sequence': array([0.86834725, 0.86834725, 0.86834725, 0.86834725, 0.86834725]), 'label': 1}, {'sequence': array([0.5206043, 0.5206043, 0.5206043, 0.5206043, 0.5206043]), 'label': 1}, {'sequence': array([0.96900771, 0.96900771, 0.96900771, 0.96900771, 0.96900771]), 'label': 1}, {'sequence': array([0.89901638, 0.89901638, 0.89901638, 0.89901638, 0.89901638]), 'label': 1}, {'sequence': array([0.98116912, 0.98116912, 0.98116912, 0.98116912, 0.98116912]), 'label': 1}, {'sequence': array([0.06144256, 0.06144256, 0.06144256, 0.06144256, 0.06144256]), 'label': 0}, {'sequence': array([0.0848691, 0.0848691, 0.0848691, 0.0848691, 0.0848691]), 'label': 0}, {'sequence': array([0.94895102, 0.94895102, 0.94895102, 0.94895102, 0.94895029]), 'label': 1}, {'sequence': array([0.46560112, 0.46560112, 0.46560112, 0.46560112, 0.46560112]), 'label': 0}, {'sequence': array([0.34895738, 0.34895738, 0.34895738, 0.34895738, 0.34895738]), 'label': 0}, {'sequence': array([0.55941606, 0.65190077, 0.65190077, 0.65190077, 0.65190077]), 'label': 1}, {'sequence': array([0.8805899, 0.8805899, 0.8805899, 0.8805899, 0.8805899]), 'label': 1}, {'sequence': array([0.62367043, 0.62367043, 0.62367043, 0.62367043, 0.62367043]), 'label': 1}, {'sequence': array([0.74850035, 0.74850035, 0.74850035, 0.74850035, 0.74850035]), 'label': 1}, {'sequence': array([0.96999399, 0.96999399, 0.96999399, 0.96999399, 0.96999399]), 'label': 1}, {'sequence': array([0.54763442, 0.54763442, 0.54763442, 0.54763442, 0.54763442]), 'label': 1}, {'sequence': array([0.38508852, 0.38508852, 0.38508852, 0.38508852, 0.38508852]), 'label': 0}, {'sequence': array([0.7622861, 0.7622861, 0.7622861, 0.7622861, 0.7622861]), 'label': 1}, {'sequence': array([0.19724714, 0.19724714, 0.19724714, 0.19724714, 0.19724714]), 'label': 0}, {'sequence': array([0.4239916, 0.4239916, 0.4239916, 0.4239916, 0.4239916]), 'label': 0}, {'sequence': array([0.06978971, 0.06978971, 0.06978971, 0.06978971, 0.06978971]), 'label': 0}, {'sequence': array([0.03114434, 0.03114434, 0.03114434, 0.03114434, 0.03114434]), 'label': 0}, {'sequence': array([0.03837099, 0.03837099, 0.03837099, 0.03837099, 0.03837099]), 'label': 0}, {'sequence': array([0.53666432, 0.53666432, 0.53666432, 0.53666432, 0.53666432]), 'label': 1}, {'sequence': array([0.57133976, 0.57133976, 0.57133976, 0.57133976, 0.57133976]), 'label': 1}, {'sequence': array([0.64670206, 0.64670206, 0.64670206, 0.64670185, 0.64670185]), 'label': 1}, {'sequence': array([0.62210414, 0.62210414, 0.62210414, 0.62210414, 0.62210414]), 'label': 1}, {'sequence': array([0.92061153, 0.92061153, 0.92061153, 0.92061153, 0.92061153]), 'label': 1}]\n",
            "<class 'list'>\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 1/10\n",
            "Trained parameters after iteration 1: [-0.08202486  0.54251979  1.00600134  0.966036   -0.01398716  1.11919778\n",
            "  0.60139379  0.39239194 -0.21677469  0.29244757  0.21505748  0.39283592\n",
            "  1.04572389  0.879534    0.78170293  0.79210524  0.73725955  0.06007305\n",
            " -0.2056647   0.75764257]\n",
            "Iteration 1 - Learning Rate: 0.243537\n",
            "Iteration 1 - Training Accuracy: 0.76\n",
            "Iteration 1 - Test Accuracy: 0.67\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 2/10\n",
            "Trained parameters after iteration 2: [-0.10276704  0.62535998  1.05567894  0.97002779 -0.0947163   1.17738503\n",
            "  0.68701981  0.37164096 -0.36486345  0.39319089  0.16132079  0.44063711\n",
            "  1.28327254  1.01746572  0.81858661  0.82620226  0.58770603  0.12009978\n",
            " -0.32273217  0.84909395]\n",
            "Iteration 2 - Learning Rate: 0.329688\n",
            "Iteration 2 - Training Accuracy: 0.84\n",
            "Iteration 2 - Test Accuracy: 0.71\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 3/10\n",
            "Trained parameters after iteration 3: [-0.17240283  0.81238044  0.96436521  1.09470366 -0.17976019  1.23680466\n",
            "  0.7912351   0.5251535  -0.55888848  0.46294243  0.15281547  0.65793992\n",
            "  1.43799583  1.1602076   1.08700044  0.76732414  0.38688514  0.12860305\n",
            " -0.31465947  0.80990751]\n",
            "Iteration 3 - Learning Rate: 0.419356\n",
            "Iteration 3 - Training Accuracy: 0.84\n",
            "Iteration 3 - Test Accuracy: 0.80\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 4/10\n",
            "Trained parameters after iteration 4: [ 0.01598327  1.04979153  0.66606024  1.25024723 -0.4781665   1.34490664\n",
            "  0.89963538  0.41830054 -0.7688611   0.17906813  0.0587285   0.71545328\n",
            "  1.53226979  1.38581595  1.02538317  0.48061245  0.1613892   0.11154221\n",
            " -0.12502851  0.83086782]\n",
            "Iteration 4 - Learning Rate: 0.530368\n",
            "Iteration 4 - Training Accuracy: 0.84\n",
            "Iteration 4 - Test Accuracy: 0.78\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 5/10\n",
            "Trained parameters after iteration 5: [ 0.19076754  1.04802393  0.46726412  1.25073832 -0.63301692  1.43874416\n",
            "  0.80970918  0.51096828 -0.86423794 -0.13829873  0.05203605  0.72325955\n",
            "  1.7719271   1.65272817  1.22442984  0.3992824   0.12792906  0.14290973\n",
            "  0.03273105  0.65079893]\n",
            "Iteration 5 - Learning Rate: 0.669124\n",
            "Iteration 5 - Training Accuracy: 0.86\n",
            "Iteration 5 - Test Accuracy: 0.81\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 6/10\n",
            "Trained parameters after iteration 6: [ 0.11822819  0.90076021  0.49369002  1.63473213 -0.82186785  1.32544101\n",
            "  0.68841614  0.64672291 -1.02520359 -0.40947132 -0.09530484  0.63269071\n",
            "  1.71966224  1.75820463  1.17437537  0.17671761  0.05507526  0.13010658\n",
            "  0.06059687  0.44024111]\n",
            "Iteration 6 - Learning Rate: 0.789835\n",
            "Iteration 6 - Training Accuracy: 0.90\n",
            "Iteration 6 - Test Accuracy: 0.81\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 7/10\n",
            "Trained parameters after iteration 7: [ 0.11398583  0.77784648  0.30089608  1.8168443  -0.92134384  1.75373015\n",
            "  0.44927539  0.71855198 -1.18969893 -0.74826909 -0.08799605  0.56580514\n",
            "  1.82685799  1.78803033  0.83005128  0.23054761  0.14401672  0.01490695\n",
            "  0.05401608  0.38893178]\n",
            "Iteration 7 - Learning Rate: 0.912290\n",
            "Iteration 7 - Training Accuracy: 0.88\n",
            "Iteration 7 - Test Accuracy: 0.76\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 8/10\n",
            "Trained parameters after iteration 8: [ 0.17876175  0.90750434  0.29952298  1.94715279 -1.07497485  1.76488805\n",
            "  0.58294852  0.83713341 -1.27753143 -0.78542495 -0.0129275   0.54843087\n",
            "  1.60976904  1.75315059  0.79536111  0.24866778  0.17229022  0.05530139\n",
            "  0.04420386  0.31198793]\n",
            "Iteration 8 - Learning Rate: 1.013470\n",
            "Iteration 8 - Training Accuracy: 0.88\n",
            "Iteration 8 - Test Accuracy: 0.73\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 9/10\n",
            "Trained parameters after iteration 9: [ 0.11466799  1.00990264  0.36149346  1.89856608 -1.07498002  1.61742926\n",
            "  0.63764171  0.89900536 -1.29599611 -0.79963749  0.01433264  0.52593021\n",
            "  1.64358846  1.88280951  0.80593611  0.27753855  0.12482062 -0.02171389\n",
            "  0.02733735  0.30649116]\n",
            "Iteration 9 - Learning Rate: 1.097824\n",
            "Iteration 9 - Training Accuracy: 0.88\n",
            "Iteration 9 - Test Accuracy: 0.73\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 10/10\n",
            "Trained parameters after iteration 10: [ 0.11685582  1.02800274  0.41399376  1.92583526 -1.09360711  1.64919356\n",
            "  0.58773339  0.98424994 -1.34780838 -0.79212285  0.03552346  0.46465848\n",
            "  1.71066027  1.8896292   0.77732369  0.26402949  0.14896962  0.00916887\n",
            "  0.01660373  0.26855994]\n",
            "Iteration 10 - Learning Rate: 1.168326\n",
            "Iteration 10 - Training Accuracy: 0.88\n",
            "Iteration 10 - Test Accuracy: 0.74\n",
            "\n",
            "\n",
            "Fed_Epoch 0, Client 2:\n",
            "Training data for epoch 0: 50\n",
            "Client Data Structure:\n",
            "[{'sequence': array([0.82116151, 0.82116151, 0.82116151, 0.82116151, 0.82116151]), 'label': 1}, {'sequence': array([0.23505501, 0.23505501, 0.23505501, 0.23505501, 0.23505501]), 'label': 0}, {'sequence': array([0.88404911, 0.88404911, 0.88404911, 0.88404911, 0.88404911]), 'label': 1}, {'sequence': array([0.27350478, 0.27350478, 0.27350478, 0.27350478, 0.27350478]), 'label': 0}, {'sequence': array([0.49842741, 0.49842741, 0.49842741, 0.49842741, 0.49842741]), 'label': 0}, {'sequence': array([0.5136558 , 0.5136529 , 0.51365   , 0.5136471 , 0.51364421]), 'label': 1}, {'sequence': array([0.4989648, 0.4989648, 0.4989648, 0.4989648, 0.4989648]), 'label': 0}, {'sequence': array([0.95254684, 0.95254684, 0.95254684, 0.95254684, 0.95254684]), 'label': 1}, {'sequence': array([0.33569417, 0.33569417, 0.33569417, 0.33569417, 0.33569417]), 'label': 0}, {'sequence': array([0.83393162, 0.83393162, 0.83393162, 0.83393162, 0.83393162]), 'label': 1}, {'sequence': array([0.32885554, 0.32885554, 0.32885554, 0.32885554, 0.32885554]), 'label': 0}, {'sequence': array([0.5136558 , 0.5136529 , 0.51365   , 0.5136471 , 0.51364421]), 'label': 1}, {'sequence': array([0.81558355, 0.81558355, 0.81558355, 0.81558355, 0.81558355]), 'label': 1}, {'sequence': array([0.54182092, 0.54182092, 0.54182092, 0.54182092, 0.54182092]), 'label': 1}, {'sequence': array([0.53991995, 0.53991995, 0.53991995, 0.53991995, 0.53991995]), 'label': 1}, {'sequence': array([0.13755082, 0.13755082, 0.13755082, 0.13755082, 0.13755082]), 'label': 0}, {'sequence': array([0.62619155, 0.62619155, 0.62619155, 0.62619155, 0.62619155]), 'label': 1}, {'sequence': array([0.0827897, 0.0827897, 0.0827897, 0.0827897, 0.0827897]), 'label': 0}, {'sequence': array([0.7468347 , 0.7468347 , 0.74683412, 0.74683412, 0.5363687 ]), 'label': 1}, {'sequence': array([0.73109255, 0.73109255, 0.73109255, 0.73109255, 0.73109255]), 'label': 1}, {'sequence': array([0.06752232, 0.06752232, 0.06752232, 0.06752232, 0.17957604]), 'label': 0}, {'sequence': array([0.70902886, 0.70902886, 0.70902886, 0.70902886, 0.70902886]), 'label': 1}, {'sequence': array([0.74946634, 0.74946634, 0.74946634, 0.74946634, 0.74946634]), 'label': 1}, {'sequence': array([0.96897118, 0.96897118, 0.96897118, 0.96897118, 0.96897118]), 'label': 1}, {'sequence': array([0.5136558 , 0.5136529 , 0.51365   , 0.5136471 , 0.51364421]), 'label': 1}, {'sequence': array([0.21819105, 0.21819105, 0.21819105, 0.21819105, 0.21819105]), 'label': 0}, {'sequence': array([0.94489565, 0.94489565, 0.94489565, 0.94489565, 0.94489565]), 'label': 1}, {'sequence': array([0.368568, 0.368568, 0.368568, 0.368568, 0.368568]), 'label': 0}, {'sequence': array([0.53302762, 0.53302762, 0.53302762, 0.53302762, 0.53302762]), 'label': 1}, {'sequence': array([0.60378941, 0.60378941, 0.60378941, 0.60378941, 0.60378941]), 'label': 1}, {'sequence': array([0.46477195, 0.46477195, 0.46477195, 0.46477195, 0.46477195]), 'label': 0}, {'sequence': array([0.10763621, 0.10763621, 0.10763621, 0.10763621, 0.10763621]), 'label': 0}, {'sequence': array([0.44973336, 0.44973336, 0.44973336, 0.44973336, 0.44973336]), 'label': 0}, {'sequence': array([0.88829595, 0.88829595, 0.88829595, 0.88829595, 0.88829595]), 'label': 1}, {'sequence': array([0.3819076, 0.3819076, 0.3819076, 0.3819076, 0.3819076]), 'label': 0}, {'sequence': array([0.22674279, 0.22674279, 0.22674279, 0.22674279, 0.22674279]), 'label': 0}, {'sequence': array([0.50627109, 0.50627109, 0.50627109, 0.50627109, 0.50627109]), 'label': 0}, {'sequence': array([0.1480568, 0.1480568, 0.1480568, 0.1480568, 0.1480568]), 'label': 0}, {'sequence': array([0.59988667, 0.59988667, 0.59988667, 0.59988667, 0.59988667]), 'label': 1}, {'sequence': array([0.61266634, 0.61266634, 0.61266634, 0.61266634, 0.61266634]), 'label': 1}, {'sequence': array([0.53957172, 0.53957172, 0.53957172, 0.53957172, 0.53957172]), 'label': 1}, {'sequence': array([0.94537803, 0.94537803, 0.94537803, 0.94537803, 0.94537803]), 'label': 1}, {'sequence': array([0.10146339, 0.10146339, 0.10146339, 0.10146339, 0.10146339]), 'label': 0}, {'sequence': array([0.97907081, 0.97907081, 0.97907081, 0.97907081, 0.97907081]), 'label': 1}, {'sequence': array([0.11582718, 0.11582718, 0.11582718, 0.11582718, 0.11582718]), 'label': 0}, {'sequence': array([0.85556794, 0.85556794, 0.85556794, 0.85556794, 0.85556794]), 'label': 1}, {'sequence': array([0.81987613, 0.81987613, 0.81987613, 0.81987613, 0.81987613]), 'label': 1}, {'sequence': array([0.7986061, 0.7986061, 0.7986061, 0.7986061, 0.7986061]), 'label': 1}, {'sequence': array([0.6070557, 0.6070557, 0.6070557, 0.6070557, 0.6070557]), 'label': 1}, {'sequence': array([0.44909227, 0.44909227, 0.44909227, 0.44909227, 0.44909227]), 'label': 0}]\n",
            "<class 'list'>\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 1/10\n",
            "Trained parameters after iteration 1: [ 1.04699433  0.28220644 -0.16016475  1.03408689 -1.26959599  1.7797753\n",
            "  0.48000246  0.52114291  0.41140204 -0.12916314 -0.12795312  1.05840475\n",
            "  0.2377784   0.37788053  0.18314493  1.41637117  1.01672955  0.76367031\n",
            "  0.96166656 -0.20256408]\n",
            "Iteration 1 - Learning Rate: 0.397540\n",
            "Iteration 1 - Training Accuracy: 0.66\n",
            "Iteration 1 - Test Accuracy: 0.65\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 2/10\n",
            "Trained parameters after iteration 2: [ 1.31459242  0.13767371 -0.09252231  1.14530314 -1.40416835  1.78813101\n",
            "  0.61429036  0.59209966  0.37490692  0.02198972 -0.05079928  1.00054165\n",
            "  0.19100426  0.31516453  0.11633843  1.28995683  0.97665681  0.78048552\n",
            "  1.05844754 -0.18818602]\n",
            "Iteration 2 - Learning Rate: 0.605213\n",
            "Iteration 2 - Training Accuracy: 0.72\n",
            "Iteration 2 - Test Accuracy: 0.72\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 3/10\n",
            "Trained parameters after iteration 3: [ 1.41095398 -0.0474275  -0.12100126  1.44022557 -1.35245685  1.64280005\n",
            "  0.72355146  0.52456806  0.45582468  0.12176422  0.00204893  1.20823224\n",
            "  0.13477658  0.3274741   0.17100986  1.28305682  1.06398461  0.66763278\n",
            "  1.11286262 -0.10139204]\n",
            "Iteration 3 - Learning Rate: 0.734743\n",
            "Iteration 3 - Training Accuracy: 0.74\n",
            "Iteration 3 - Test Accuracy: 0.73\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 4/10\n",
            "Trained parameters after iteration 4: [ 1.77650582 -0.24295017 -0.04611834  1.40466895 -1.09823149  1.89843823\n",
            "  1.01855555  0.38664866  0.26005817  0.3999663   0.1476299   1.44863353\n",
            " -0.03892567  0.23366306  0.0594353   1.4612043   1.09041917  0.47023926\n",
            "  1.50960849  0.05146286]\n",
            "Iteration 4 - Learning Rate: 0.884695\n",
            "Iteration 4 - Training Accuracy: 0.76\n",
            "Iteration 4 - Test Accuracy: 0.69\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 5/10\n",
            "Trained parameters after iteration 5: [ 1.71127429e+00 -7.18902108e-02  4.31154874e-02  1.46403904e+00\n",
            " -1.47000702e+00  1.57149014e+00  8.32979010e-01  6.65095086e-01\n",
            " -2.52601799e-01  3.60092828e-01  3.73047096e-01  1.59686638e+00\n",
            " -3.03860301e-01 -3.98503723e-02  1.98866907e-01  1.38409808e+00\n",
            "  1.08375830e+00 -1.66295790e-03  1.33721776e+00 -1.25985852e-01]\n",
            "Iteration 5 - Learning Rate: 1.099704\n",
            "Iteration 5 - Training Accuracy: 0.76\n",
            "Iteration 5 - Test Accuracy: 0.70\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 6/10\n",
            "Trained parameters after iteration 6: [ 2.01227368  0.05251887  0.51798968  0.89026166 -0.82864812  1.82974087\n",
            "  1.25190777 -0.51927083 -0.62634817  0.08913638 -0.43527732  1.92711002\n",
            " -0.51506253 -0.02329052  0.13051375  1.83153938  1.35610729 -0.05105876\n",
            "  0.95677593  0.18168661]\n",
            "Iteration 6 - Learning Rate: 1.472911\n",
            "Iteration 6 - Training Accuracy: 0.82\n",
            "Iteration 6 - Test Accuracy: 0.69\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 7/10\n",
            "Trained parameters after iteration 7: [ 3.19824443 -0.64526411  0.48201352  1.84425297 -1.07322959  2.18820205\n",
            "  1.51444008 -0.78146959 -0.55169317  0.7286717   0.346132    1.28536636\n",
            " -0.19198861 -0.14262593 -0.2197215   2.8005118   1.17866766  0.12215372\n",
            "  1.63569798  0.82988572]\n",
            "Iteration 7 - Learning Rate: 2.098249\n",
            "Iteration 7 - Training Accuracy: 0.76\n",
            "Iteration 7 - Test Accuracy: 0.67\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 8/10\n",
            "Trained parameters after iteration 8: [ 1.3729559  -0.44808866  1.4246783   2.76667943 -1.55436022  2.77030312\n",
            "  2.58354115 -0.5029333  -0.61099608  0.88216822  1.3505398   1.64832043\n",
            "  1.11132256 -0.46265701  0.60984402  1.07600934  0.93865841 -0.84020196\n",
            "  2.77144141 -0.10397325]\n",
            "Iteration 8 - Learning Rate: 2.760805\n",
            "Iteration 8 - Training Accuracy: 0.78\n",
            "Iteration 8 - Test Accuracy: 0.78\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 9/10\n",
            "Trained parameters after iteration 9: [ 0.96996176  0.48892364  3.14884164  2.88980713 -1.14383001  3.13679195\n",
            "  4.47555059 -1.17437908 -0.06019127  1.89285321 -0.81611322  4.11681056\n",
            "  1.46373043 -1.14550865  1.43563972 -0.09364754  2.59944945 -3.18920482\n",
            "  3.71676202  0.28173354]\n",
            "Iteration 9 - Learning Rate: 3.677261\n",
            "Iteration 9 - Training Accuracy: 0.66\n",
            "Iteration 9 - Test Accuracy: 0.54\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 10/10\n",
            "Trained parameters after iteration 10: [ 0.24596784  0.57175512  6.16040993  0.78192672 -3.93594632  5.69017554\n",
            "  0.26352513 -3.76101984  0.15760384 -0.36483738 -5.2675379   5.81223567\n",
            "  3.02351948  0.78197707  1.28966142  1.72826404  1.96983697 -2.6152338\n",
            "  0.70133438 -1.75787491]\n",
            "Iteration 10 - Learning Rate: 4.983024\n",
            "Iteration 10 - Training Accuracy: 0.58\n",
            "Iteration 10 - Test Accuracy: 0.48\n",
            "\n",
            "\n",
            "Fed_Epoch 0, Client 3:\n",
            "Training data for epoch 0: 50\n",
            "Client Data Structure:\n",
            "[{'sequence': array([0.87100251, 0.87100251, 0.87100251, 0.78395075, 0.87100041]), 'label': 1}, {'sequence': array([0.36621451, 0.36621451, 0.36621451, 0.36621451, 0.36621451]), 'label': 0}, {'sequence': array([0.97060103, 0.97060103, 0.97060103, 0.97060103, 0.97060103]), 'label': 1}, {'sequence': array([0.00879736, 0.00879736, 0.00879736, 0.00879736, 0.00879736]), 'label': 0}, {'sequence': array([0.5136558 , 0.5136529 , 0.51365   , 0.5136471 , 0.51364421]), 'label': 1}, {'sequence': array([0.75605827, 0.75605827, 0.75605827, 0.75605827, 0.75605827]), 'label': 1}, {'sequence': array([0.64838677, 0.64838677, 0.64838677, 0.64838677, 0.64838677]), 'label': 1}, {'sequence': array([0.03051651, 0.03051651, 0.03051651, 0.03051651, 0.03051651]), 'label': 0}, {'sequence': array([0.93561127, 0.93561127, 0.93561127, 0.93561127, 0.93561127]), 'label': 1}, {'sequence': array([0.22384032, 0.22384032, 0.22384032, 0.22384032, 0.22384032]), 'label': 0}, {'sequence': array([0.23837899, 0.23837899, 0.23837899, 0.23837899, 0.23837899]), 'label': 0}, {'sequence': array([0.7036776, 0.7036776, 0.7036776, 0.7036776, 0.7036776]), 'label': 1}, {'sequence': array([0.95599967, 0.95599967, 0.95599967, 0.95599967, 0.95599967]), 'label': 1}, {'sequence': array([0.11613041, 0.11613041, 0.11613041, 0.11613041, 0.11613041]), 'label': 0}, {'sequence': array([0.46404302, 0.46404302, 0.46404302, 0.46404302, 0.46404302]), 'label': 0}, {'sequence': array([0.64718756, 0.64718756, 0.64718756, 0.64718756, 0.64718756]), 'label': 1}, {'sequence': array([0.28129795, 0.28129795, 0.28129795, 0.28129795, 0.28129795]), 'label': 0}, {'sequence': array([0.94679495, 0.94679495, 0.94679495, 0.94679495, 0.94679495]), 'label': 1}, {'sequence': array([0.61693848, 0.61693812, 0.61693812, 0.61693812, 0.61693812]), 'label': 1}, {'sequence': array([0.67624823, 0.67624823, 0.67624823, 0.67624823, 0.67624823]), 'label': 1}, {'sequence': array([0.78223698, 0.78223698, 0.78223698, 0.78223698, 0.78223698]), 'label': 1}, {'sequence': array([0.49264029, 0.49264029, 0.49264029, 0.49264029, 0.49264029]), 'label': 0}, {'sequence': array([0.33706681, 0.33706681, 0.33706681, 0.33706681, 0.33706681]), 'label': 0}, {'sequence': array([0.58375685, 0.56976594, 0.58375656, 0.58375656, 0.58375656]), 'label': 1}, {'sequence': array([0.49232859, 0.49232859, 0.49232859, 0.49232859, 0.49232859]), 'label': 0}, {'sequence': array([0.25956387, 0.25956387, 0.25956387, 0.25956387, 0.25956263]), 'label': 0}, {'sequence': array([0.49881115, 0.49881115, 0.49881115, 0.49881115, 0.49881115]), 'label': 0}, {'sequence': array([0.08118515, 0.08118515, 0.08118515, 0.08118515, 0.08118515]), 'label': 0}, {'sequence': array([0.79249148, 0.79249148, 0.79249148, 0.79249148, 0.79249148]), 'label': 1}, {'sequence': array([0.95122747, 0.95122747, 0.95122747, 0.95122747, 0.95122747]), 'label': 1}, {'sequence': array([0.52273616, 0.52273616, 0.52273616, 0.52273616, 0.52273616]), 'label': 1}, {'sequence': array([0.21280782, 0.21280782, 0.21280782, 0.21280782, 0.21280782]), 'label': 0}, {'sequence': array([0.57313477, 0.57313245, 0.51365   , 0.5136471 , 0.57312803]), 'label': 1}, {'sequence': array([0.64864594, 0.64864594, 0.64864594, 0.64864594, 0.64864594]), 'label': 1}, {'sequence': array([0.6723128, 0.6723128, 0.6723128, 0.6723128, 0.6723128]), 'label': 1}, {'sequence': array([0.74796246, 0.74796246, 0.74796246, 0.74796246, 0.74796246]), 'label': 1}, {'sequence': array([0.13858096, 0.13858096, 0.13858096, 0.13858096, 0.13858096]), 'label': 0}, {'sequence': array([0.61706343, 0.61706343, 0.51709334, 0.61706205, 0.51709334]), 'label': 1}, {'sequence': array([0.78932991, 0.78932991, 0.78932991, 0.78932991, 0.78932991]), 'label': 1}, {'sequence': array([0.5136558 , 0.5136529 , 0.51365   , 0.5136471 , 0.51364421]), 'label': 1}, {'sequence': array([0.40591081, 0.40591081, 0.40591081, 0.40591081, 0.40591081]), 'label': 0}, {'sequence': array([0.20495914, 0.20495914, 0.20495914, 0.20495914, 0.20495914]), 'label': 0}, {'sequence': array([0.69869899, 0.69869899, 0.69869899, 0.69869899, 0.69869899]), 'label': 1}, {'sequence': array([0.49351903, 0.49351903, 0.49351903, 0.49351903, 0.49351903]), 'label': 0}, {'sequence': array([0.11724483, 0.11724483, 0.11724483, 0.11724483, 0.11724483]), 'label': 0}, {'sequence': array([0.63793551, 0.63793551, 0.63793529, 0.63793529, 0.63793529]), 'label': 1}, {'sequence': array([0.61821118, 0.61821118, 0.61821118, 0.61821118, 0.61821118]), 'label': 1}, {'sequence': array([0.49330422, 0.49330422, 0.49330422, 0.49330422, 0.49330422]), 'label': 0}, {'sequence': array([0.6743411, 0.6743411, 0.6743411, 0.6743411, 0.6743411]), 'label': 1}, {'sequence': array([0.05454754, 0.05454754, 0.05454754, 0.05454754, 0.05454754]), 'label': 0}]\n",
            "<class 'list'>\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 1/10\n",
            "Trained parameters after iteration 1: [-0.27663801 -0.55028894  0.29031614  3.78153291  1.46545839  0.42200861\n",
            " -1.18699145  2.10655229  1.95761707  1.10721319  1.92457827  3.55227999\n",
            "  1.59637646 -1.71275059  1.19666413  0.22271068 -0.63533364 -0.17284501\n",
            "  4.04265157  3.0510495 ]\n",
            "Iteration 1 - Learning Rate: 1.563121\n",
            "Iteration 1 - Training Accuracy: 0.40\n",
            "Iteration 1 - Test Accuracy: 0.38\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 2/10\n",
            "Trained parameters after iteration 2: [-1.08929729 -1.41675893  0.71827762  3.46314611  1.40204216  0.5247414\n",
            " -0.65374936  2.1717886   1.57758003  1.46743348  2.37373651  3.11439241\n",
            "  1.28463198 -0.99970746  1.04242446  0.27621264 -1.10166647 -1.40169495\n",
            "  4.61313532  3.5741448 ]\n",
            "Iteration 2 - Learning Rate: 2.713130\n",
            "Iteration 2 - Training Accuracy: 0.68\n",
            "Iteration 2 - Test Accuracy: 0.62\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 3/10\n",
            "Trained parameters after iteration 3: [-1.3942919  -3.45596047 -2.0894686   2.17331814  1.50701993  0.87040701\n",
            "  2.69528851 -0.21631821  0.66370835  2.38220479  0.73049187  1.95195275\n",
            "  3.65528554 -2.18018752 -0.82875516  0.29833978 -2.65721165  0.33159764\n",
            "  6.42724364  2.78728877]\n",
            "Iteration 3 - Learning Rate: 3.888211\n",
            "Iteration 3 - Training Accuracy: 0.46\n",
            "Iteration 3 - Test Accuracy: 0.65\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 4/10\n",
            "Trained parameters after iteration 4: [ 1.35475777 -1.2719951  -7.28522108  3.92908579  1.49473218  0.77370431\n",
            "  4.06135451 -3.82852959  1.48288368 -0.61737342  0.8058323   1.05194111\n",
            "  3.18638755 -0.95901977  4.29768458  4.88642479 -2.66891367  0.24377066\n",
            "  8.12654831 -1.72469058]\n",
            "Iteration 4 - Learning Rate: 5.638950\n",
            "Iteration 4 - Training Accuracy: 0.62\n",
            "Iteration 4 - Test Accuracy: 0.48\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 5/10\n",
            "Trained parameters after iteration 5: [-1.42840617 -0.45419853 -8.14765641  2.57810539  1.63891645  0.14460957\n",
            "  1.51203638 -4.28095182  2.15117398  1.99018657  2.26811422  2.44800148\n",
            "  5.61318028 -3.31730072  2.60460752  8.85528726 -3.29178656  2.68751067\n",
            "  8.1060922  -2.16245902]\n",
            "Iteration 5 - Learning Rate: 7.404022\n",
            "Iteration 5 - Training Accuracy: 0.46\n",
            "Iteration 5 - Test Accuracy: 0.48\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 6/10\n",
            "Trained parameters after iteration 6: [-1.25152314 -4.73873215 -5.61202276  0.95970586  6.10735686  2.7072106\n",
            " -6.23755871 -4.3606287   4.17243901  7.38225373 -2.42499587  2.74970659\n",
            "  9.36442836  1.76003843  2.79015568 17.99114752 -5.62477448  6.00154116\n",
            " 11.48809368  1.98797346]\n",
            "Iteration 6 - Learning Rate: 9.819114\n",
            "Iteration 6 - Training Accuracy: 0.44\n",
            "Iteration 6 - Test Accuracy: 0.34\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 7/10\n",
            "Trained parameters after iteration 7: [ -5.93228828  -0.68053584  -3.67592044  11.2291003    5.37250125\n",
            "  -9.00045274  -7.35354186   1.28191264  11.09620195   9.13444037\n",
            " -10.99084271   6.57069516   1.82923084   4.18531477  -7.27458042\n",
            "   3.75529205   4.94983042  12.620076     3.38044793  -4.57712802]\n",
            "Iteration 7 - Learning Rate: 14.400639\n",
            "Iteration 7 - Training Accuracy: 0.42\n",
            "Iteration 7 - Test Accuracy: 0.42\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 8/10\n",
            "Trained parameters after iteration 8: [ -2.45596635   4.31010427  -1.29496575  16.06405622   8.41647435\n",
            "  -8.20527308  -9.13339298  18.56644371   9.83210848  18.15858249\n",
            "  -8.29366687   0.76204167   2.94714923  -2.65844841 -15.50225314\n",
            "   6.4742003   -1.34130182  11.03020611   4.15633894  -7.33432215]\n",
            "Iteration 8 - Learning Rate: 19.243871\n",
            "Iteration 8 - Training Accuracy: 0.50\n",
            "Iteration 8 - Test Accuracy: 0.48\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 9/10\n",
            "Trained parameters after iteration 9: [-11.2218242    3.83218747  -3.18650583  18.89481723  15.13501151\n",
            " -16.1092649  -10.75247545  18.33493534  20.67700947  14.35986933\n",
            "  13.88364657   7.36691782  -1.20703345  -9.57648025 -15.5109278\n",
            "   4.93412426  14.10227815 -12.04578871  -1.14701935   0.03078435]\n",
            "Iteration 9 - Learning Rate: 26.567963\n",
            "Iteration 9 - Training Accuracy: 0.56\n",
            "Iteration 9 - Test Accuracy: 0.51\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 10/10\n",
            "Trained parameters after iteration 10: [-18.81948838   3.24706422   5.21082557   4.62356385  34.15183606\n",
            " -29.72206727  -7.20839809   6.19947288  26.40130266  25.6265394\n",
            "   5.13894439  29.4299276  -17.91275277 -22.51511574 -12.44610145\n",
            "  22.74017914   9.67366075 -20.35586993 -21.94807253 -20.42728135]\n",
            "Iteration 10 - Learning Rate: 35.109825\n",
            "Iteration 10 - Training Accuracy: 0.34\n",
            "Iteration 10 - Test Accuracy: 0.40\n",
            "\n",
            "\n",
            "Fed_Epoch 0, Client 4:\n",
            "Training data for epoch 0: 50\n",
            "Client Data Structure:\n",
            "[{'sequence': array([0.54436233, 0.54436233, 0.54436233, 0.54436233, 0.54436233]), 'label': 1}, {'sequence': array([0.16015378, 0.16015378, 0.16015378, 0.16015378, 0.16015378]), 'label': 0}, {'sequence': array([0.85549887, 0.85549887, 0.85549887, 0.85549887, 0.85549887]), 'label': 1}, {'sequence': array([0.9207251, 0.9207251, 0.9207251, 0.9207251, 0.9207251]), 'label': 1}, {'sequence': array([0.69224984, 0.69224984, 0.69224984, 0.69224984, 0.69224984]), 'label': 1}, {'sequence': array([0.48786142, 0.48786142, 0.48786142, 0.48786142, 0.48786142]), 'label': 0}, {'sequence': array([0.59253102, 0.59253102, 0.59253102, 0.59253102, 0.59253102]), 'label': 1}, {'sequence': array([0.1556922, 0.1556922, 0.1556922, 0.1556922, 0.1556922]), 'label': 0}, {'sequence': array([0.70824565, 0.70824565, 0.70824565, 0.70824565, 0.70824565]), 'label': 1}, {'sequence': array([0.77090734, 0.77090734, 0.77090734, 0.77090734, 0.77090734]), 'label': 1}, {'sequence': array([0.45249712, 0.45249712, 0.45249712, 0.45249712, 0.45249712]), 'label': 0}, {'sequence': array([0.44798597, 0.44798597, 0.44798597, 0.44798597, 0.44798597]), 'label': 0}, {'sequence': array([0.55166957, 0.55166957, 0.55166957, 0.55166957, 0.55166957]), 'label': 1}, {'sequence': array([0.15010504, 0.15010504, 0.15010504, 0.15010504, 0.15010504]), 'label': 0}, {'sequence': array([0.85913506, 0.85913506, 0.85913506, 0.85913506, 0.85913506]), 'label': 1}, {'sequence': array([0.66056746, 0.66056746, 0.66056746, 0.66056746, 0.66056746]), 'label': 1}, {'sequence': array([0.06594052, 0.06594052, 0.06594052, 0.06594052, 0.06594052]), 'label': 0}, {'sequence': array([0.34981452, 0.34981452, 0.34981452, 0.34981452, 0.34981452]), 'label': 0}, {'sequence': array([0.77657597, 0.77657597, 0.77657597, 0.77657597, 0.77657597]), 'label': 1}, {'sequence': array([0.28696186, 0.28696186, 0.28696186, 0.28696186, 0.28696186]), 'label': 0}, {'sequence': array([0.92812734, 0.92812734, 0.92812734, 0.92812734, 0.92812734]), 'label': 1}, {'sequence': array([0.32131994, 0.32131994, 0.32131994, 0.32131994, 0.32131994]), 'label': 0}, {'sequence': array([0.71954325, 0.71954325, 0.71954325, 0.71954325, 0.71954325]), 'label': 1}, {'sequence': array([0.42947281, 0.42947281, 0.42947281, 0.42947281, 0.42947281]), 'label': 0}, {'sequence': array([0.00567333, 0.00567333, 0.00567333, 0.00567333, 0.00567333]), 'label': 0}, {'sequence': array([0.66593887, 0.66593887, 0.66593887, 0.66593887, 0.66593887]), 'label': 1}, {'sequence': array([0.48220838, 0.48220838, 0.48220838, 0.48220838, 0.48220838]), 'label': 0}, {'sequence': array([0.93058867, 0.93058867, 0.93058867, 0.93058867, 0.93058867]), 'label': 1}, {'sequence': array([0.07869208, 0.13599207, 0.13599207, 0.13599207, 0.13599207]), 'label': 0}, {'sequence': array([0.23209688, 0.23209463, 0.23209587, 0.23209587, 0.23208557]), 'label': 0}, {'sequence': array([0.46407063, 0.46407063, 0.46407063, 0.46407063, 0.46407063]), 'label': 0}, {'sequence': array([0.51415985, 0.51415985, 0.51415985, 0.51415985, 0.51415985]), 'label': 1}, {'sequence': array([0.56928087, 0.56928087, 0.56928087, 0.56928087, 0.56928087]), 'label': 1}, {'sequence': array([0.03212744, 0.03212744, 0.03212744, 0.03212744, 0.03212744]), 'label': 0}, {'sequence': array([0.50210092, 0.50210092, 0.50210092, 0.50210092, 0.50210092]), 'label': 0}, {'sequence': array([0.49731661, 0.49731661, 0.49731661, 0.49731661, 0.49731661]), 'label': 0}, {'sequence': array([0.68157927, 0.68157927, 0.68157927, 0.68157927, 0.68157927]), 'label': 1}, {'sequence': array([0.50496665, 0.50496665, 0.50496665, 0.50496665, 0.50496665]), 'label': 0}, {'sequence': array([0.95697574, 0.95697574, 0.95697574, 0.95697574, 0.95697574]), 'label': 1}, {'sequence': array([0.64368704, 0.64368704, 0.64368704, 0.64368704, 0.64368704]), 'label': 1}, {'sequence': array([0.9204618, 0.9204618, 0.9204618, 0.9204618, 0.9204618]), 'label': 1}, {'sequence': array([0.58809407, 0.58809407, 0.58809407, 0.58809407, 0.58809125]), 'label': 1}, {'sequence': array([0.40585508, 0.40585508, 0.40585508, 0.40585508, 0.40585508]), 'label': 0}, {'sequence': array([0.3570566, 0.3570566, 0.3570566, 0.3570566, 0.3570566]), 'label': 0}, {'sequence': array([0.76585605, 0.76585605, 0.76585605, 0.76585605, 0.76585605]), 'label': 1}, {'sequence': array([0.85480871, 0.85480871, 0.85480871, 0.85480871, 0.85480871]), 'label': 1}, {'sequence': array([0.92091041, 0.92091041, 0.92091041, 0.92091041, 0.92091041]), 'label': 1}, {'sequence': array([0.73022816, 0.73022816, 0.73022816, 0.73022816, 0.73022816]), 'label': 1}, {'sequence': array([0.59790554, 0.59790554, 0.55529989, 0.53612208, 0.59790025]), 'label': 1}, {'sequence': array([0.71378715, 0.71378715, 0.71378715, 0.71378715, 0.71378715]), 'label': 1}]\n",
            "<class 'list'>\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 1/10\n",
            "Trained parameters after iteration 1: [ -6.64525092   1.61740058 -12.87441535   8.00946753  -2.35262114\n",
            "   7.21386314  19.70977808  -8.08488637  12.33771072 -42.6004677\n",
            " -22.85547695   3.33839251  -9.93723159  13.66355667  13.50878614\n",
            "  -2.86594917   7.87483637 -25.95827149  -9.86329787  -5.71786419]\n",
            "Iteration 1 - Learning Rate: 11.618214\n",
            "Iteration 1 - Training Accuracy: 0.54\n",
            "Iteration 1 - Test Accuracy: 0.59\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 2/10\n",
            "Trained parameters after iteration 2: [ -4.9008931    7.98477371 -14.53928303   7.85236298  -2.24964112\n",
            "  13.10839797  19.83146775  -7.20470729  17.64725001 -41.18405403\n",
            " -19.78837029   5.05029867 -22.5975749   13.05075312  15.14135023\n",
            "  -9.33102646  14.63182868 -34.71378174  -7.69903493 -11.14130331]\n",
            "Iteration 2 - Learning Rate: 20.997712\n",
            "Iteration 2 - Training Accuracy: 0.54\n",
            "Iteration 2 - Test Accuracy: 0.42\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 3/10\n",
            "Trained parameters after iteration 3: [ -9.20717743   4.50404057 -17.42485026  -2.24945037  -1.85393968\n",
            "   2.83150857  17.26416616  -4.521107    15.33796088 -48.28057389\n",
            " -17.38246356   8.93074862 -21.2360074    0.73904113  21.77142389\n",
            " -14.69850035   7.91844244 -25.62361373  -7.22681313   8.15234551]\n",
            "Iteration 3 - Learning Rate: 29.228643\n",
            "Iteration 3 - Training Accuracy: 0.52\n",
            "Iteration 3 - Test Accuracy: 0.48\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 4/10\n",
            "Trained parameters after iteration 4: [ -8.39214689  19.3797889  -31.70728177 -12.55085185 -11.98550503\n",
            "  22.7884063    0.63043348   3.10101482  12.11211866 -46.6536425\n",
            " -27.67761288   2.51795094 -25.77682745  -3.6289845   40.84544372\n",
            " -45.23524713   4.69820999 -36.35252371 -29.26396214  26.72035493]\n",
            "Iteration 4 - Learning Rate: 39.556828\n",
            "Iteration 4 - Training Accuracy: 0.68\n",
            "Iteration 4 - Test Accuracy: 0.61\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 5/10\n",
            "Trained parameters after iteration 5: [ -4.50028138  16.71220795 -42.11017715   1.95402596 -42.00836671\n",
            "  29.81386315  -3.9369963    2.48544184  -6.57695453 -72.03109804\n",
            " -18.84407255  -9.14546934 -15.3315922    4.18932628  48.27425254\n",
            " -13.1030074    7.36292408 -45.35063886 -18.5785271  -14.95541061]\n",
            "Iteration 5 - Learning Rate: 53.155267\n",
            "Iteration 5 - Training Accuracy: 0.46\n",
            "Iteration 5 - Test Accuracy: 0.40\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 6/10\n",
            "Trained parameters after iteration 6: [ 39.840975    10.8956596  -38.16147636  31.90520849 -47.50584796\n",
            "  55.62957059  49.5079188   -9.1419567   16.88356759 -34.19523123\n",
            "  -5.65137869 -36.42489782  -3.66228482  37.7414466   23.0111946\n",
            "  44.0346795   16.17872058 -74.92202367  14.87700294   2.61881761]\n",
            "Iteration 6 - Learning Rate: 74.169334\n",
            "Iteration 6 - Training Accuracy: 0.44\n",
            "Iteration 6 - Test Accuracy: 0.55\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 7/10\n"
          ]
        }
      ],
      "source": [
        "# Initialize a global list to track global loss\n",
        "global_loss_per_round = []\n",
        "\n",
        "# Display information about the data assigned to each client, including epoch-wise splits\n",
        "for idx, client in enumerate(clients):\n",
        "    print(f\"Client {idx + 1}:\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"  Epoch {epoch + 1}: Train data samples: {len(client.data[epoch])}\")\n",
        "    print(f\"  Test data samples: {len(client.test_data)}\")\n",
        "\n",
        "# Display information about the data assigned to each client\n",
        "for idx, client in enumerate(clients):\n",
        "    print(f\"Client {idx + 1}:\")\n",
        "    print(f\"  Train data epochs: {len(client.data)}\")\n",
        "    print(f\"  Test data samples: {len(client.test_data)}\")\n",
        "\n",
        "    # Accessing the number of features in a sequence\n",
        "    if client.data:\n",
        "        num_features=client.data[0][0]['sequence'].shape[0]  # Access first data point of epoch 0\n",
        "        #num_features = client.data[0]['sequence'].shape[0]\n",
        "        print(f\"  Number of features in a sequence: {num_features}\")\n",
        "\n",
        "def reset_state():\n",
        "    # Reset the objective value, learning rate, and perturbation after each client\n",
        "    global objective_func_vals, learning_rates, perturbations\n",
        "    objective_func_vals = []  # Reset objective values\n",
        "    learning_rates = []  # Reset learning rates\n",
        "    perturbations = []  # Reset perturbations\n",
        "# Function to reset callback graph state after each round\n",
        "def reset_callback_graph():\n",
        "    global gradient_moving_avg, learning_rates, perturbations\n",
        "\n",
        "    # Reset the state variables to start fresh for the next round\n",
        "    gradient_moving_avg = np.zeros_like(gradient_moving_avg)  # Reset gradient moving average\n",
        "    learning_rates = [initial_learning_rate]  # Reset learning rates list to initial value\n",
        "    perturbations = [initial_perturbation]  # Reset perturbations list to initial value\n",
        "import csv\n",
        "\n",
        "# Path to store the best client's data\n",
        "#best_client_csv_file = '/content/drive/My Drive/Best_Client_DQFL_Genome_IID_22_04_2025_Teleport.csv'\n",
        "\n",
        "\n",
        "\n",
        "# Update CSV headers to include round time\n",
        "best_headers = [\"Federated Round\", \"Client Number\", \"Round Duration (s)\"]\n",
        "with open(best_client_csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(best_headers)\n",
        "\n",
        "# Function to update the best client data\n",
        "def save_best_client_results(federated_round, best_client_index, round_duration):\n",
        "    with open(best_client_csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([federated_round,\n",
        "                         best_client_index,\n",
        "                         f\"{round_duration:.4f}\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Clear the CSV file for a new run\n",
        "clear_csv_file()\n",
        "\n",
        "\n",
        "# --- Before starting  federated loop: initialize the CSV with a header ---\n",
        "with open(validation_csv_file, mode='w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Round', 'Central Validation Loss'])\n",
        "\n",
        "validation_loss_per_round = []\n",
        "\n",
        "# Wrap the epoch loop with tqdm\n",
        "for epoch in tqdm(range(num_federated_layers), desc=\"Training Progress\", leave =True):\n",
        "    round_start = time.time()\n",
        "\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_train_accuracies, epoch_test_accuracies = [], []\n",
        "    best_client_index = -1\n",
        "    best_client_accuracy = -1\n",
        "    best_client_model = None\n",
        "    print(\"\\n\")\n",
        "    print(f\"Fed_Epoch: {epoch}\")\n",
        "    round_losses = []  # Track individual client losses for this round\n",
        "\n",
        "    for index, client in enumerate(clients):\n",
        "        print(\"\\n\")\n",
        "        print(f\"Fed_Epoch {epoch}, Client {index + 1}:\")\n",
        "        reset_state()\n",
        "\n",
        "        try:\n",
        "            # Ensure you're using the correct index for data\n",
        "            current_data = client.data[epoch]  # This assumes data is structured in epochs\n",
        "            print(f\"Training data for epoch {epoch}: {len(current_data)}\")\n",
        "        except IndexError:\n",
        "            print(f\"No data available for epoch {epoch} for Client {index + 1}\")\n",
        "            continue  # Skip this client for the current epoch\n",
        "\n",
        "        model, train_score, test_score, train_time = train_qnn_model(\n",
        "            client.data[epoch],\n",
        "            client.test_data,\n",
        "            client_id=index,\n",
        "            layer=epoch,\n",
        "        )\n",
        "\n",
        "        epoch_train_accuracies.append(train_score)\n",
        "        epoch_test_accuracies.append(test_score)\n",
        "\n",
        "        # Fetch the client's loss (assumes train_qnn_model returns it)\n",
        "        #current_loss = objective_func_vals[-1]  # Fetch latest loss\n",
        "        #round_losses.append(current_loss)\n",
        "        # Calculate global loss for the current round as the average of client losses\n",
        "\n",
        "\n",
        "        # Check if this client has the best accuracy so far\n",
        "        if test_score > best_client_accuracy:\n",
        "            best_client_accuracy = test_score\n",
        "            best_client_index = index\n",
        "            best_client_model = model  # Directly store the best client's model\n",
        "\n",
        "    #global_loss = sum(round_losses) / len(round_losses)\n",
        "    #global_loss_per_round.append(global_loss)  # Store the global loss\n",
        "\n",
        "    #print(f\"Global Loss for Round {epoch}: {global_loss}\")\n",
        "\n",
        "    round_duration = time.time() - round_start\n",
        "    round_times.append(round_duration)\n",
        "    print(f\"Time for Round {epoch}: {round_duration:.2f} s\")\n",
        "\n",
        "    #Save best client *and* round duration to CSV\n",
        "    save_best_client_results(epoch, best_client_index, round_duration)\n",
        "  # Save to best client CSV\n",
        "    print(f\"Best client for epoch {epoch} is Client {best_client_index + 1} with test accuracy {best_client_accuracy:.2f}\")\n",
        "\n",
        "    #-----------------------------------------------------------------Teleportation------------------------------------------------\n",
        "\n",
        "    ## --- Quantum Teleportation-based Model Update Transfer ---\n",
        "    # Extract, teleport, and update the global model parameters.\n",
        "    # Choose quantum update transmission scheme:\n",
        "    #use_teleportation = True  # Set to False to simulate pure entanglement transfer\n",
        "\n",
        "\n",
        "    # Treat the best client's model as the global model for the next round\n",
        "    #Extract\n",
        "    global_model = best_client_model\n",
        "    global_params = global_model.weights\n",
        "    #Teleport\n",
        "    if use_teleportation:\n",
        "        updated_params = teleport_parameters(global_params, show_histogram=False, use_noise=False)\n",
        "    else:\n",
        "        updated_params = entanglement_parameters(global_params, show_histogram=False, use_noise=True)\n",
        "\n",
        "    global_model.initial_point = updated_params\n",
        "\n",
        "    # Update all clients with the global model\n",
        "    for index, client in enumerate(clients):\n",
        "        client.primary_model = global_model\n",
        "\n",
        "    # Evaluate the global model on the new test data\n",
        "    global_accuracy = get_accuracy(global_model, test_sequences, test_labels)\n",
        "    global_model_accuracy.append(global_accuracy)\n",
        "\n",
        "    clients_train_accuracies.append(epoch_train_accuracies)\n",
        "    clients_test_accuracies.append(epoch_test_accuracies)\n",
        "\n",
        "    print(f\"Global Model Accuracy in Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    # evaluate central validation\n",
        "    L_val = compute_validation_loss(global_model, X_val, y_val)\n",
        "    validation_loss_per_round.append(L_val)\n",
        "    print(f\"After round {epoch}, central val loss = {L_val:.4f}\\n\")\n",
        "\n",
        "    # --- Append the latest validation loss to the CSV ---\n",
        "    with open(validation_csv_file, mode='a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([epoch, L_val])\n",
        "\n",
        "\n",
        "\n",
        "    # Save results for the current iteration of the client in the federated round\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Step 1: Mount Google Drive\n",
        "    #drive.mount('/content/drive')\n",
        "\n",
        "    # Step 2: Define the save path in Google Drive\n",
        "    #save_path = '/content/drive/MyDrive/DQFL_Genome_IID_Global__22_04_2025_Teleport.csv'\n",
        "\n",
        "\n",
        "    # Save accuracies to CSV after each epoch (or at the end of all epochs)\n",
        "    save_accuracies_to_csv(global_model_accuracy, clients_train_accuracies, clients_test_accuracies, filename=global_csv_file)\n",
        "    # After each round, reset callback state to prepare for the next round\n",
        "    reset_callback_graph()\n",
        "    print(f\"File saved to {global_csv_file}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Saved validation loss per round to {validateLoss_csv_file}\")\n",
        "# 10) Plot validation trajectory\n",
        "plt.plot(validation_loss_per_round, marker='o')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Central Validation Loss')\n",
        "plt.title('Federated QFL Validation Loss vs Round')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#print(\"Accuracy data saved to\", csv_file_path)\n",
        "# After loop, compute total elapsed time\n",
        "total_training_time = time.time() - overall_start\n",
        "print(f\"Total training time: {total_training_time:.2f} s\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RwQoPVIRKbI"
      },
      "outputs": [],
      "source": [
        "# Define the path to save global loss\n",
        "global_loss_csv = '/content/drive/My Drive/Federated_Global_Loss_22_04_2025.csv'\n",
        "\n",
        "# Write headers to the CSV file (only at the beginning)\n",
        "if epoch == 0:\n",
        "    with open(global_loss_csv, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Federated Round\", \"Global Loss\"])\n",
        "\n",
        "# Append the global loss after each round\n",
        "with open(global_loss_csv, mode='a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([epoch, global_loss])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D9XnmHoRPez"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot global loss over federated rounds\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(global_loss_per_round)), global_loss_per_round, marker='o', color='blue', label=\"Global Loss\")\n",
        "plt.xlabel(\"Federated Round\")\n",
        "plt.ylabel(\"Global Loss\")\n",
        "plt.title(\"Global Loss Over Federated Rounds\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zs_rlJT5XJv"
      },
      "source": [
        "Split data as iid and non-iid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pgm1g3VHfXC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Introduce custome cross entropy function\n",
        "import numpy as np\n",
        "\n",
        "# Callback for updating learning rate dynamically with deep unfolding principles\n",
        "def deep_unfolding_learning_rate_adjustment(obj_func_eval, gradients=None, client_id=None, layer=None):\n",
        "    global gradient_moving_avg, learning_rates, perturbations,meta_alpha, meta_epsilon, momentum\n",
        "\n",
        "    # Initialize moving average for gradients\n",
        "    if gradients is not None:\n",
        "        if not isinstance(gradient_moving_avg, np.ndarray) or gradient_moving_avg.size == 0:\n",
        "            gradient_moving_avg = gradients\n",
        "        else:\n",
        "            # Update moving average of gradients (Momentum)\n",
        "            gradient_moving_avg = momentum * gradient_moving_avg + (1 - momentum) * gradients\n",
        "        # Calculate the average gradient\n",
        "        avg_gradient = np.mean(gradient_moving_avg)\n",
        "\n",
        "        # Normalize delta_lr by L2 norm of the gradient\n",
        "        norm_gradient = np.linalg.norm(gradients)\n",
        "\n",
        "        '''\n",
        "        # Normalization to prevent instability\n",
        "        norm_gradient = gradients / (np.linalg.norm(gradients) + 1e-8)\n",
        "        avg_gradient = np.mean(norm_gradient)\n",
        "        '''\n",
        "        # Trainable scaling for deep unfolding (meta-parameter)\n",
        "        meta_alpha = 0.01  # This can be learned via a hypernetwork or meta-learning\n",
        "        meta_epsilon = 1e-6  # Small offset to ensure numerical stability\n",
        "        # Gradually adjust learning rate based on gradient signs and magnitude\n",
        "        # This formula gradually adds or subtracts from the learning rate instead of multiplication\n",
        "        delta_lr = meta_alpha * np.sign(avg_gradient) * np.sqrt(np.abs(avg_gradient) + meta_epsilon) / (norm_gradient + 1e-6)\n",
        "\n",
        "        #delta_lr = meta_alpha * np.sign(avg_gradient) * np.sqrt(np.abs(avg_gradient) + meta_epsilon)\n",
        "    # Apply gradual adjustment (either addition or subtraction based on the direction of the gradient)\n",
        "        if avg_gradient > 0:\n",
        "            delta_lr = delta_lr - 0.001  # Decrease if gradient is positive (potential overfitting)\n",
        "        else:\n",
        "            delta_lr = delta_lr + 0.001  # Increase if gradient is negative (potential underfitting)\n",
        "    else:\n",
        "        delta_lr = 0\n",
        "\n",
        "    # Compute new learning rate with clamping for stability\n",
        "    new_lr = max(0.001, min(5.0, learning_rates[-1] + delta_lr)) if learning_rates else initial_learning_rate\n",
        "\n",
        "    # Update per-client, per-layer information if federated\n",
        "    if client_id is not None and layer is not None:\n",
        "        if client_id not in client_data:\n",
        "            client_data[client_id] = {'federated_layers': {}}\n",
        "        if layer not in client_data[client_id]['federated_layers']:\n",
        "            client_data[client_id]['federated_layers'][layer] = {'objective_values': [], 'learning_rates': []}\n",
        "\n",
        "        # Store loss and learning rate for the specific client and layer\n",
        "        client_data[client_id]['federated_layers'][layer]['objective_values'].append(obj_func_eval)\n",
        "        client_data[client_id]['federated_layers'][layer]['learning_rates'].append(new_lr)\n",
        "\n",
        "    # Store global metrics\n",
        "    objective_func_vals.append(obj_func_eval)  # Store the loss value globally\n",
        "    learning_rates.append(new_lr)  # Append the new learning rate to the history\n",
        "\n",
        "    # Update meta-parameters (meta_alpha and meta_epsilon) using gradient descent\n",
        "    #meta_gradients = compute_meta_gradients(gradients, avg_gradient, delta_lr)\n",
        "    #meta_alpha -= meta_learning_rate * meta_gradients['alpha']\n",
        "    #meta_epsilon -= meta_learning_rate * meta_gradients['epsilon']\n",
        "\n",
        "    # Debug output for analysis\n",
        "    # print(f\"Objective Function Value: {obj_func_eval:.6f}, New Learning Rate: {new_lr:.6f}\")\n",
        "\n",
        "    return new_lr\n",
        "\n",
        "\n",
        "def callback_graph(weights, loss):\n",
        "    \"\"\"Callback to log and synchronize loss during training.\"\"\"\n",
        "    #print(f\"Loss = {loss}\")\n",
        "    if len(objective_func_vals) == 0 or loss != objective_func_vals[-1]:\n",
        "        objective_func_vals.append(loss)\n",
        "\n",
        "spsa_optimizer = SPSA(maxiter=50, learning_rate=0.01, perturbation = 0.15, callback=lambda nfev, params, obj_func_eval, stepsize, accept: deep_unfolding_learning_rate_adjustment(obj_func_eval, stepsize))\n",
        "\n",
        "\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file_path = 'federated_learning_accuracy.csv'\n",
        "\n",
        "# Open the CSV file in write mode and add headers (if starting fresh)\n",
        "with open(csv_file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write the header\n",
        "    writer.writerow(['Epoch', 'Global Accuracy'] + [f'Client {i+1} Final Accuracy' for i in range(num_clients)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a_q7UqBTcju"
      },
      "source": [
        "new ways to average"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}